{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with H2O - Tutorial 4a: Classification Models (Basics)\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Objective**:\n",
    "\n",
    "- This tutorial explains how to build classification models with four different H2O algorithms.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Titanic Dataset:**\n",
    "\n",
    "- Source: https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "<hr>\n",
    "    \n",
    "**Algorithms**:\n",
    "\n",
    "1. GLM\n",
    "2. DRF\n",
    "3. GBM\n",
    "4. DNN\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Full Technical Reference:**\n",
    "\n",
    "- http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_131\"; OpenJDK Runtime Environment (build 1.8.0_131-8u131-b11-0ubuntu1.16.04.2-b11); OpenJDK 64-Bit Server VM (build 25.131-b11, mixed mode)\n",
      "  Starting server from /home/joe/anaconda3/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpq3s4s6qw\n",
      "  JVM stdout: /tmp/tmpq3s4s6qw/h2o_joe_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpq3s4s6qw/h2o_joe_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.10.5.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>10 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_joe_8n99xp</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>5.210 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.1 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster version:        3.10.5.2\n",
       "H2O cluster version age:    10 days\n",
       "H2O cluster name:           H2O_from_python_joe_8n99xp\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    5.210 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "Python version:             3.6.1 final\n",
       "--------------------------  ------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start and connect to a local H2O cluster\n",
    "import h2o\n",
    "h2o.init(nthreads = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  PassengerId</th><th style=\"text-align: right;\">  Survived</th><th style=\"text-align: right;\">  Pclass</th><th>Name                                               </th><th>Sex   </th><th style=\"text-align: right;\">  Age</th><th style=\"text-align: right;\">  SibSp</th><th style=\"text-align: right;\">  Parch</th><th style=\"text-align: right;\">  Ticket</th><th style=\"text-align: right;\">   Fare</th><th>Cabin  </th><th>Embarked  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">       3</td><td>Braund, Mr. Owen Harris                            </td><td>male  </td><td style=\"text-align: right;\">   22</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 7.25  </td><td>       </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       1</td><td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td><td>female</td><td style=\"text-align: right;\">   38</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">71.2833</td><td>C85    </td><td>C         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       3</td><td>Heikkinen, Miss. Laina                             </td><td>female</td><td style=\"text-align: right;\">   26</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\"> 7.925 </td><td>       </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            4</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">       1</td><td>Futrelle, Mrs. Jacques Heath (Lily May Peel)       </td><td>female</td><td style=\"text-align: right;\">   35</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  113803</td><td style=\"text-align: right;\">53.1   </td><td>C123   </td><td>S         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">            5</td><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">       3</td><td>Allen, Mr. William Henry                           </td><td>male  </td><td style=\"text-align: right;\">   35</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  373450</td><td style=\"text-align: right;\"> 8.05  </td><td>       </td><td>S         </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Titanic data (local CSV)\n",
    "titanic = h2o.import_file(\"kaggle_titanic.csv\")\n",
    "titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert 'Survived' and 'Pclass' to categorical values\n",
    "titanic['Survived'] = titanic['Survived'].asfactor()\n",
    "titanic['Pclass'] = titanic['Pclass'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  Survived</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">         0</td><td style=\"text-align: right;\">    549</td></tr>\n",
       "<tr><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">    342</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  Pclass</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">    216</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       2</td><td style=\"text-align: right;\">    184</td></tr>\n",
       "<tr><td style=\"text-align: right;\">       3</td><td style=\"text-align: right;\">    491</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Pclass'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Sex   </th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>female</td><td style=\"text-align: right;\">    314</td></tr>\n",
       "<tr><td>male  </td><td style=\"text-align: right;\">    577</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Sex'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56ed245eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['Age'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZEBIkjoZEJKkTv8f4ynXNhUUYKoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56e774e518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['SibSp'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGVxJREFUeJzt3Xu0nXV95/H3B1DucikxEwka1BQHbAWMVMZLVWqhoxKc\n1WIcL9Gh0o5462VZcDlVV1dmMV2KWh1cpqgTrxhQLmprB1Ktl6ohKBYSYBG5DAmBpChigIYSvvPH\n/h3dxCfn7INnn31y8n6tddZ+nt9z+z4ncD77uf2eVBWSJO1oj1EXIEmamQwISVInA0KS1MmAkCR1\nMiAkSZ0MCElSJwNCM1aStUleMOo6RinJy5PcnmRrkuNGWMetSX5nVNvXaBgQGomuPzhJXpfkW2Pj\nVXVMVX19gvUsSFJJ9hpSqaP2XuBNVXVAVf1gx4lt3+9rAbIxyXlJ9hxBnZqFDAhpHDMgeJ4ErJ1g\nnmdU1QHAScB/Bd4w2Y3MgP3UDGRAaMbqP8pIckKSNUnuTXJXkvPabN9on/e0b9EnJtkjyTuT3JZk\nc5JPJjmob72vbdPuTvI/dtjOu5NcnOTTSe4FXte2/Z0k9yTZlOTDSR7bt75K8sYkNyX5WZK/SvKU\nJP/c6l3ZP/8O+9hZa5K9k2wF9gR+mORHE/2+quoG4JvA09u6z07yo1bTuiQv79vu65J8O8n7k9wN\nvLu1vyHJ9X3LHN+3iWOT/EuSnyb5fJJ9JqpJuzYDQruKDwIfrKrHAU8BVrb257fPg9tpmO8Ar2s/\nLwSeDBwAfBggydHA+cCrgHnAQcDhO2xrMXAxcDDwGWA78CfAYcCJ9L6pv3GHZU4Gngk8G3g7sBx4\nNXAEvT/Yr9zJfnXWWlXb2lEB9I4QnrLzX01P27fnAWOnon7Uxg8C3gN8Osm8vkV+C7gZmAssS/IH\n9ILitcDjgFOBu/vmPx04BTgS+M1Wt2YxA0KjdGn7Vn5Pknvo/eHemX8HnprksKraWlXfHWfeVwHn\nVdXNVbUVOAdY0k6j/D7wpar6VlU9CPwlsGOHZN+pqkur6uGqeqCqrq6q71bVQ1V1K/BR4Ld3WOav\nq+reqloLXAf837b9nwJ/D+zsAvN4tQ7q+0l+AnwJuAD4BEBVXVRVd7T9+DxwE3BC33J3VNWH2n49\nAPxh24+rqmd9Vd3WN//ftPX9uG3r2EnUqF2QAaFROq2qDh774Ze/lfc7A/h14IYkVyV56TjzPgHo\n/8N2G7AXvW/KTwBuH5tQVffzyG/J9E8HSPLrSb6c5M522ul/0jua6HdX3/ADHeMH0G28Wgd1fFUd\nUlVPqap3VtXDre7XJrmmL4CfvkPdt++wniPoHXXszJ19w/ez833SLGFAaJdQVTdV1SuBxwP/C7g4\nyf788rd/gDvoXdwd80TgIXp/tDcB88cmJNkX+LUdN7fD+EeAG4CF7RTXO4A8+r0ZuNZHLcmTgL8F\n3gT8Wgvg63hk3Tvu5+30Tt9JgAGhXUSSVyeZ074d39OaHwa2tM8n983+OeBPkhyZ5AB63/g/X1UP\n0bu28LIk/6ldOH43E/+xPxC4F9ia5GnAf5+q/Zqg1l/FWHhuAUjyetrF63FcAPx5kmem56ktaLSb\nMiC0qzgFWNvu7PkgsKRdH7gfWAZ8u51KeTbwceBT9O5wugX4N+DNAO0awZuBC+kdTWwFNgPbxtn2\nn9O7ffRn9L6Vf34K92untf4qqmod8D7gO/SORn4D+PYEy1xE73f5WXr7eilw6K9ai3Zd8YVB2p21\nb+330Dt9dMuo65FmEo8gtNtJ8rIk+7VrGO8FrgVuHW1V0sxjQGh3tJjexeE7gIX0Tld5KC3twFNM\nkqROHkFIkjrt0h10HXbYYbVgwYJRlyFJu5Srr776X6tqzkTzDS0gkhzFI28HfDK9bg0+2doX0Lsw\neHpV/aQtcw69J2a3A2+pqn8YbxsLFixgzZo1U167JM1mSW6beK4hnmKqqhur6tiqOpZeJ2b3A5cA\nZwOrqmohsKqNj3U0tgQ4ht497+fbr70kjc50XYM4CfhR6/hrMbCita8ATmvDi4ELWy+WtwDreWTH\nYpKkaTRdAbGEXpcCAHOralMbvpNfdEp2OI/sPGwDv9wNM0nObO8FWLNly5Zh1StJu72hB0Tr7+ZU\n4KIdp7V7zyd1n21VLa+qRVW1aM6cCa+xSJIepek4gvg94PtVNdY75V1jLy1pn5tb+0Z63Q2Pmd/a\nJEkjMB0B8Up+cXoJ4HJgaRteClzW176kvWrxSHpPuK6ehvokSR2G+hxE6+vmxcAf9TWfC6xMcga9\nl6OcDr1eNpOsBNbR6w//rKraPsz6JEk7N9SAqKr72OFlLFV1N727mrrmX0avu2FJ0ojZ1YYkqdMu\n3dXGr2rB2V8ZyXZvPfclI9muJE2GRxCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqZEBI\nkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMBIUnqNNSA\nSHJwkouT3JDk+iQnJjk0yRVJbmqfh/TNf06S9UluTHLyMGuTJI1v2EcQHwS+WlVPA54BXA+cDayq\nqoXAqjZOkqOBJcAxwCnA+Un2HHJ9kqSdGFpAJDkIeD7wMYCqerCq7gEWAyvabCuA09rwYuDCqtpW\nVbcA64EThlWfJGl8wzyCOBLYAnwiyQ+SXJBkf2BuVW1q89wJzG3DhwO39y2/obVJkkZgmAGxF3A8\n8JGqOg64j3Y6aUxVFVCTWWmSM5OsSbJmy5YtU1asJOmRhhkQG4ANVfW9Nn4xvcC4K8k8gPa5uU3f\nCBzRt/z81vYIVbW8qhZV1aI5c+YMrXhJ2t0NLSCq6k7g9iRHtaaTgHXA5cDS1rYUuKwNXw4sSbJ3\nkiOBhcDqYdUnSRrfXkNe/5uBzyR5LHAz8Hp6obQyyRnAbcDpAFW1NslKeiHyEHBWVW0fcn2SpJ0Y\nakBU1TXAoo5JJ+1k/mXAsmHWJEkajE9SS5I6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMB\nIUnqZEBIkjoZEJKkTgaEJKmTASFJ6mRASJI6GRCSpE4GhCSpkwEhSepkQEiSOhkQkqROBoQkqZMB\nIUnqZEBIkjoZEJKkTkMNiCS3Jrk2yTVJ1rS2Q5NckeSm9nlI3/znJFmf5MYkJw+zNknS+KbjCOKF\nVXVsVS1q42cDq6pqIbCqjZPkaGAJcAxwCnB+kj2noT5JUodRnGJaDKxowyuA0/raL6yqbVV1C7Ae\nOGEE9UmSGH5AFHBlkquTnNna5lbVpjZ8JzC3DR8O3N637IbW9ghJzkyyJsmaLVu2DKtuSdrt7TXk\n9T+3qjYmeTxwRZIb+idWVSWpyaywqpYDywEWLVo0qWUlSYMb6hFEVW1sn5uBS+idMroryTyA9rm5\nzb4ROKJv8fmtTZI0AkMLiCT7JzlwbBj4XeA64HJgaZttKXBZG74cWJJk7yRHAguB1cOqT5I0vmGe\nYpoLXJJkbDufraqvJrkKWJnkDOA24HSAqlqbZCWwDngIOKuqtg+xPknSOIYWEFV1M/CMjva7gZN2\nsswyYNmwapIkDc4nqSVJnQwISVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJ\nUicDQpLUyYCQJHUyICRJnQwISVKngQIiyW8MuxBJ0swy6BHE+UlWJ3ljkoOGWpEkaUYYKCCq6nnA\nq+i9M/rqJJ9N8uKhViZJGqmBr0FU1U3AO4G/AH4b+JskNyT5L8MqTpI0OoNeg/jNJO8HrgdeBLys\nqv5jG37/EOuTJI3IoO+k/hBwAfCOqnpgrLGq7kjyzqFUJkkaqUED4iXAA1W1HSDJHsA+VXV/VX1q\naNVJkkZm0GsQVwL79o3v19omlGTPJD9I8uU2fmiSK5Lc1D4P6Zv3nCTrk9yY5ORBd0KSNPUGDYh9\nqmrr2Egb3m/AZd9K79rFmLOBVVW1EFjVxklyNLAEOAY4hd6ttXsOuA1J0hQbNCDuS3L82EiSZwIP\njDP/2Hzz6Z2euqCveTGwog2vAE7ra7+wqrZV1S3AeuCEAeuTJE2xQa9BvA24KMkdQID/ALxigOU+\nALwdOLCvbW5VbWrDdwJz2/DhwHf75tvQ2iRJIzBQQFTVVUmeBhzVmm6sqn8fb5kkLwU2V9XVSV6w\nk/VWkppMwUnOBM4EeOITnziZRSVJkzDoEQTAs4AFbZnjk1BVnxxn/ucApyb5z8A+wOOSfBq4K8m8\nqtqUZB6wuc2/kd6T2mPmt7ZHqKrlwHKARYsWTSpcJEmDG/RBuU8B7wWeSy8ongUsGm+ZqjqnquZX\n1QJ6F5//sapeDVwOLG2zLQUua8OXA0uS7J3kSGAhsHpyuyNJmiqDHkEsAo6uqqn4xn4usDLJGcBt\nwOkAVbU2yUpgHfAQcNbYcxeSpOk3aEBcR+/C9KaJZuxSVV8Hvt6G7wZO2sl8y4Blj2YbkqSpNWhA\nHAasS7Ia2DbWWFWnDqUqSdLIDRoQ7x5mEZKkmWfQ21z/KcmTgIVVdWWS/QCfcpakWWzQu5jeAFwM\nfLQ1HQ5cOqyiJEmjN2hXG2fRe67hXvj5y4MeP6yiJEmjN2hAbKuqB8dGkuwF+JCaJM1igwbEPyV5\nB7Bvexf1RcCXhleWJGnUBg2Is4EtwLXAHwF/R+/91JKkWWrQu5geBv62/UiSdgMDBUSSW+i45lBV\nT57yiiRJM8Jk+mIasw/wB8ChU1+OJGmmGOgaRFXd3fezsao+QO9NcZKkWWrQU0zH943uQe+IYjLv\nkpAk7WIG/SP/vr7hh4Bbad10S5Jmp0HvYnrhsAuRJM0sg55i+tPxplfVeVNTjiRpppjMXUzPovda\nUICX0Xsd6E3DKEqSNHqDBsR84Piq+hlAkncDX2nvmJYkzUKDdrUxF3iwb/zB1iZJmqUGPYL4JLA6\nySVt/DRgxXBKkiTNBIPexbQsyd8Dz2tNr6+qHwyvLEnSqA16iglgP+DeqvogsCHJkUOqSZI0Awz6\nytF3AX8BnNOaHgN8eoJl9kmyOskPk6xN8p7WfmiSK5Lc1D4P6VvmnCTrk9yY5ORHt0uSpKkw6BHE\ny4FTgfsAquoO4MAJltkGvKiqngEcC5yS5Nn03i2xqqoWAqvaOEmOBpYAxwCnAOcn2XNyuyNJmiqD\nBsSDVVW0Lr+T7D/RAtWztY0+pv0UsJhfXOBeQe+CN639wqraVlW3AOuBEwasT5I0xQYNiJVJPgoc\nnOQNwJUM8PKgJHsmuQbYDFxRVd8D5lbVpjbLnfzidtnDgdv7Ft/Q2nZc55lJ1iRZs2XLlgHLlyRN\n1qB3Mb23vYv6XuAo4C+r6ooBltsOHJvkYOCSJE/fYXol+aUXEU2wzuXAcoBFixZNallJ0uAmDIh2\nHeDK1mHfhKHQparuSfI1etcW7koyr6o2JZlH7+gCYCNwRN9i81ubJGkEJjzF1I4CHk5y0GRWnGRO\nO3Igyb7Ai4Eb6PXntLTNthS4rA1fDixJsne7hXYhvf6eJEkjMOiT1FuBa5NcQbuTCaCq3jLOMvOA\nFe0IZA9gZVV9Ocl36F3TOAO4jfZeiapam2QlsI7eOyfOauEkSRqBQQPii+1nYFX1L8BxHe13Ayft\nZJllwLLJbEeSNBzjBkSSJ1bV/6sq+12SpN3MRNcgLh0bSPKFIdciSZpBJgqI9A0/eZiFSJJmlokC\nonYyLEma5Sa6SP2MJPfSO5LYtw3TxquqHjfU6iRJIzNuQFSVneVJ0m5qMu+DkCTtRgwISVInA0KS\n1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVInA0KS\n1MmAkCR1GlpAJDkiydeSrEuyNslbW/uhSa5IclP7PKRvmXOSrE9yY5KTh1WbJGliwzyCeAj4s6o6\nGng2cFaSo4GzgVVVtRBY1cZp05YAxwCnAOcn8Y12kjQiQwuIqtpUVd9vwz8DrgcOBxYDK9psK4DT\n2vBi4MKq2lZVtwDrgROGVZ8kaXzTcg0iyQLgOOB7wNyq2tQm3QnMbcOHA7f3Lbahte24rjOTrEmy\nZsuWLUOrWZJ2d0MPiCQHAF8A3lZV9/ZPq6oCajLrq6rlVbWoqhbNmTNnCiuVJPUbakAkeQy9cPhM\nVX2xNd+VZF6bPg/Y3No3Akf0LT6/tUmSRmCYdzEF+BhwfVWd1zfpcmBpG14KXNbXviTJ3kmOBBYC\nq4dVnyRpfHsNcd3PAV4DXJvkmtb2DuBcYGWSM4DbgNMBqmptkpXAOnp3QJ1VVduHWJ8kaRxDC4iq\n+haQnUw+aSfLLAOWDasmSdLgfJJaktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVIn\nA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnYb2Tmrt\n3IKzvzKS7d567ktGsl1JuyaPICRJnYYWEEk+nmRzkuv62g5NckWSm9rnIX3TzkmyPsmNSU4eVl2S\npMEM8wji/wCn7NB2NrCqqhYCq9o4SY4GlgDHtGXOT7LnEGuTJE1gaAFRVd8AfrxD82JgRRteAZzW\n135hVW2rqluA9cAJw6pNkjSx6b4GMbeqNrXhO4G5bfhw4Pa++Ta0tl+S5Mwka5Ks2bJly/AqlaTd\n3MguUldVAfUollteVYuqatGcOXOGUJkkCaY/IO5KMg+gfW5u7RuBI/rmm9/aJEkjMt0BcTmwtA0v\nBS7ra1+SZO8kRwILgdXTXJskqc/QHpRL8jngBcBhSTYA7wLOBVYmOQO4DTgdoKrWJlkJrAMeAs6q\nqu3Dqk2SNLGhBURVvXInk07ayfzLgGXDqkeSNDl2taFpYfci0q7HrjYkSZ0MCElSJwNCktTJgJAk\ndTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnQwISVInA0KS1MmAkCR1MiAkSZ0MCElSJwNCktTJgJAk\ndTIgJEmdDAhJUicDQpLUyYCQJHUyICRJnWZcQCQ5JcmNSdYnOXvU9UjS7mqvURfQL8mewP8GXgxs\nAK5KcnlVrRttZdLkLTj7KyPZ7q3nvmQk290djerfGKbn33lGBQRwArC+qm4GSHIhsBgwIKRdgKE4\nu6SqRl3DzyX5feCUqvrDNv4a4Leq6k1985wJnNlGjwJunIbSDgP+dRq2Mwqzed/A/dvVuX/D8aSq\nmjPRTDPtCGJCVbUcWD6d20yypqoWTec2p8ts3jdw/3Z17t9ozbSL1BuBI/rG57c2SdI0m2kBcRWw\nMMmRSR4LLAEuH3FNkrRbmlGnmKrqoSRvAv4B2BP4eFWtHXFZMM2ntKbZbN43cP92de7fCM2oi9SS\npJljpp1ikiTNEAaEJKmTATGOJB9PsjnJdaOuZaolOSLJ15KsS7I2yVtHXdNUSrJPktVJftj27z2j\nrmmqJdkzyQ+SfHnUtQxDkluTXJvkmiRrRl3PVEpycJKLk9yQ5PokJ466pi5egxhHkucDW4FPVtXT\nR13PVEoyD5hXVd9PciBwNXDabOnWJEmA/atqa5LHAN8C3lpV3x1xaVMmyZ8Ci4DHVdVLR13PVEty\nK7Coqmbdg3JJVgDfrKoL2h2b+1XVPaOua0ceQYyjqr4B/HjUdQxDVW2qqu+34Z8B1wOHj7aqqVM9\nW9voY9rPrPk2lGQ+8BLgglHXoslJchDwfOBjAFX14EwMBzAgBCRZABwHfG+0lUytdgrmGmAzcEVV\nzab9+wDwduDhURcyRAVcmeTq1sXObHEksAX4RDtFeEGS/UddVBcDYjeX5ADgC8DbqureUdczlapq\ne1UdS++J/BOSzIrThEleCmyuqqtHXcuQPbf9+/0ecFY75Tsb7AUcD3ykqo4D7gNm5KsNDIjdWDs3\n/wXgM1X1xVHXMyzt8P1rwCmjrmWKPAc4tZ2jvxB4UZJPj7akqVdVG9vnZuASer09zwYbgA19R7QX\n0wuMGceA2E21i7gfA66vqvNGXc9USzInycFteF967xi5YbRVTY2qOqeq5lfVAnrd0fxjVb16xGVN\nqST7t5snaKdffheYFXcTVtWdwO1JjmpNJzFDX2kwo7ramGmSfA54AXBYkg3Au6rqY6Otaso8B3gN\ncG07Tw/wjqr6uxHWNJXmASvaS6j2AFZW1ay8HXSWmgtc0vsew17AZ6vqq6MtaUq9GfhMu4PpZuD1\nI66nk7e5SpI6eYpJktTJgJAkdTIgJEmdDAhJUicDQpLUyYCQBpBke+tV9LokFyXZbwrW+bokH56K\n+qRhMCCkwTxQVce2Xn0fBP540AXbsxjSLseAkCbvm8BTAZJc2jqTW9vfoVySrUnel+SHwIlJnpXk\nn9v7KVaPPSUMPCHJV5PclOSvR7Av0k75JLU0CUn2otd53NhTvf+tqn7cuvO4KskXqupuYH/ge1X1\nZ+1p2RuAV1TVVUkeBzzQlj+WXk+624Abk3yoqm6f1p2SdsKAkAazb1+XJN+k9eUPvCXJy9vwEcBC\n4G5gO72OEAGOAjZV1VUAY73mtm4kVlXVT9v4OuBJgAGhGcGAkAbzQOt6+ueSvAD4HeDEqro/ydeB\nfdrkf6uq7QOsd1vf8Hb8f1IziNcgpEfvIOAnLRyeBjx7J/PdCMxL8iyAJAe2U1XSjOZ/pNKj91Xg\nj5NcTy8EOt93XVUPJnkF8KF2reIBekce0oxmb66SpE6eYpIkdTIgJEmdDAhJUicDQpLUyYCQJHUy\nICRJnQwISVKn/w8/EaKCSnCabQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56ed24ec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['Parch'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56e769d7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic['Fare'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Embarked  </th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>C         </td><td style=\"text-align: right;\">    168</td></tr>\n",
       "<tr><td>Q         </td><td style=\"text-align: right;\">     77</td></tr>\n",
       "<tr><td>S         </td><td style=\"text-align: right;\">    644</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Embarked'].table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define features (or predictors) manually\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the H2O data frame into training/test sets\n",
    "# so we can evaluate out-of-bag performance\n",
    "titanic_split = titanic.split_frame(ratios = [0.8], seed = 1234)\n",
    "\n",
    "titanic_train = titanic_split[0] # using 80% for training\n",
    "titanic_test = titanic_split[1]  # using the rest 20% for out-of-bag evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Generalized Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a Generalized Linear Model (GLM) with default settings\n",
    "\n",
    "# Import the function for GLM\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "\n",
    "# Set up GLM for binary classification\n",
    "glm_default = H2OGeneralizedLinearEstimator(family = 'binomial', model_id = 'glm_default')\n",
    "\n",
    "# Use .train() to build the model\n",
    "glm_default.train(x = features, \n",
    "                  y = 'Survived', \n",
    "                  training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Modeling\n",
      "Model Key:  glm_default\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1382288206898215\n",
      "RMSE: 0.37179136715343664\n",
      "LogLoss: 0.4365971276608109\n",
      "Null degrees of freedom: 711\n",
      "Residual degrees of freedom: 700\n",
      "Null deviance: 939.9987544934203\n",
      "Residual deviance: 621.7143097889948\n",
      "AIC: 645.7143097889948\n",
      "AUC: 0.8541387024608501\n",
      "Gini: 0.7082774049217002\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.326408717958219: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>354.0</td>\n",
       "<td>93.0</td>\n",
       "<td>0.2081</td>\n",
       "<td> (93.0/447.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>54.0</td>\n",
       "<td>211.0</td>\n",
       "<td>0.2038</td>\n",
       "<td> (54.0/265.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>408.0</td>\n",
       "<td>304.0</td>\n",
       "<td>0.2065</td>\n",
       "<td> (147.0/712.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      354  93   0.2081   (93.0/447.0)\n",
       "1      54   211  0.2038   (54.0/265.0)\n",
       "Total  408  304  0.2065   (147.0/712.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3264087</td>\n",
       "<td>0.7416520</td>\n",
       "<td>231.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2662247</td>\n",
       "<td>0.7879656</td>\n",
       "<td>251.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6655020</td>\n",
       "<td>0.8023379</td>\n",
       "<td>122.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6655020</td>\n",
       "<td>0.8146067</td>\n",
       "<td>122.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9710368</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0497483</td>\n",
       "<td>1.0</td>\n",
       "<td>384.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9710368</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.6655020</td>\n",
       "<td>0.6016370</td>\n",
       "<td>122.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3403702</td>\n",
       "<td>0.7919463</td>\n",
       "<td>230.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.3264087</td>\n",
       "<td>0.7940864</td>\n",
       "<td>231.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.326409     0.741652  231\n",
       "max f2                       0.266225     0.787966  251\n",
       "max f0point5                 0.665502     0.802338  122\n",
       "max accuracy                 0.665502     0.814607  122\n",
       "max precision                0.971037     1         0\n",
       "max recall                   0.0497483    1         384\n",
       "max specificity              0.971037     1         0\n",
       "max absolute_mcc             0.665502     0.601637  122\n",
       "max min_per_class_accuracy   0.34037      0.791946  230\n",
       "max mean_per_class_accuracy  0.326409     0.794086  231"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 37.22 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0112360</td>\n",
       "<td>0.9610609</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.0301887</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0210674</td>\n",
       "<td>0.9552300</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0566038</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0308989</td>\n",
       "<td>0.9519189</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0830189</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0407303</td>\n",
       "<td>0.9421320</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1094340</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0505618</td>\n",
       "<td>0.9334337</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1358491</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1011236</td>\n",
       "<td>0.8705719</td>\n",
       "<td>2.5375262</td>\n",
       "<td>2.6121593</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.9722222</td>\n",
       "<td>0.1283019</td>\n",
       "<td>0.2641509</td>\n",
       "<td>153.7526205</td>\n",
       "<td>161.2159329</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1502809</td>\n",
       "<td>0.7799310</td>\n",
       "<td>2.6100270</td>\n",
       "<td>2.6114618</td>\n",
       "<td>0.9714286</td>\n",
       "<td>0.9719626</td>\n",
       "<td>0.1283019</td>\n",
       "<td>0.3924528</td>\n",
       "<td>161.0026954</td>\n",
       "<td>161.1461823</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2008427</td>\n",
       "<td>0.7067333</td>\n",
       "<td>1.9404612</td>\n",
       "<td>2.4425386</td>\n",
       "<td>0.7222222</td>\n",
       "<td>0.9090909</td>\n",
       "<td>0.0981132</td>\n",
       "<td>0.4905660</td>\n",
       "<td>94.0461216</td>\n",
       "<td>144.2538593</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3005618</td>\n",
       "<td>0.5934800</td>\n",
       "<td>1.5515280</td>\n",
       "<td>2.1469229</td>\n",
       "<td>0.5774648</td>\n",
       "<td>0.7990654</td>\n",
       "<td>0.1547170</td>\n",
       "<td>0.6452830</td>\n",
       "<td>55.1528036</td>\n",
       "<td>114.6922941</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4002809</td>\n",
       "<td>0.4039384</td>\n",
       "<td>1.1731066</td>\n",
       "<td>1.9043231</td>\n",
       "<td>0.4366197</td>\n",
       "<td>0.7087719</td>\n",
       "<td>0.1169811</td>\n",
       "<td>0.7622642</td>\n",
       "<td>17.3106564</td>\n",
       "<td>90.4323072</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.2226229</td>\n",
       "<td>0.6811587</td>\n",
       "<td>1.6603774</td>\n",
       "<td>0.2535211</td>\n",
       "<td>0.6179775</td>\n",
       "<td>0.0679245</td>\n",
       "<td>0.8301887</td>\n",
       "<td>-31.8841350</td>\n",
       "<td>66.0377358</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5997191</td>\n",
       "<td>0.1387461</td>\n",
       "<td>0.4919479</td>\n",
       "<td>1.4660952</td>\n",
       "<td>0.1830986</td>\n",
       "<td>0.5456674</td>\n",
       "<td>0.0490566</td>\n",
       "<td>0.8792453</td>\n",
       "<td>-50.8052086</td>\n",
       "<td>46.6095179</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7022472</td>\n",
       "<td>0.1172022</td>\n",
       "<td>0.2944430</td>\n",
       "<td>1.2950340</td>\n",
       "<td>0.1095890</td>\n",
       "<td>0.482</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.9094340</td>\n",
       "<td>-70.5556991</td>\n",
       "<td>29.5033962</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7991573</td>\n",
       "<td>0.0923965</td>\n",
       "<td>0.3115122</td>\n",
       "<td>1.1757668</td>\n",
       "<td>0.1159420</td>\n",
       "<td>0.4376098</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.9396226</td>\n",
       "<td>-68.8487832</td>\n",
       "<td>17.5766820</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8988764</td>\n",
       "<td>0.0763351</td>\n",
       "<td>0.4541058</td>\n",
       "<td>1.0957075</td>\n",
       "<td>0.1690141</td>\n",
       "<td>0.4078125</td>\n",
       "<td>0.0452830</td>\n",
       "<td>0.9849057</td>\n",
       "<td>-54.5894233</td>\n",
       "<td>9.5707547</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0101444</td>\n",
       "<td>0.1492662</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.3721910</td>\n",
       "<td>0.0150943</td>\n",
       "<td>1.0</td>\n",
       "<td>-85.0733753</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.011236                    0.961061           2.68679   2.68679            1                1                           0.0301887       0.0301887                  168.679   168.679\n",
       "    2        0.0210674                   0.95523            2.68679   2.68679            1                1                           0.0264151       0.0566038                  168.679   168.679\n",
       "    3        0.0308989                   0.951919           2.68679   2.68679            1                1                           0.0264151       0.0830189                  168.679   168.679\n",
       "    4        0.0407303                   0.942132           2.68679   2.68679            1                1                           0.0264151       0.109434                   168.679   168.679\n",
       "    5        0.0505618                   0.933434           2.68679   2.68679            1                1                           0.0264151       0.135849                   168.679   168.679\n",
       "    6        0.101124                    0.870572           2.53753   2.61216            0.944444         0.972222                    0.128302        0.264151                   153.753   161.216\n",
       "    7        0.150281                    0.779931           2.61003   2.61146            0.971429         0.971963                    0.128302        0.392453                   161.003   161.146\n",
       "    8        0.200843                    0.706733           1.94046   2.44254            0.722222         0.909091                    0.0981132       0.490566                   94.0461   144.254\n",
       "    9        0.300562                    0.59348            1.55153   2.14692            0.577465         0.799065                    0.154717        0.645283                   55.1528   114.692\n",
       "    10       0.400281                    0.403938           1.17311   1.90432            0.43662          0.708772                    0.116981        0.762264                   17.3107   90.4323\n",
       "    11       0.5                         0.222623           0.681159  1.66038            0.253521         0.617978                    0.0679245       0.830189                   -31.8841  66.0377\n",
       "    12       0.599719                    0.138746           0.491948  1.4661             0.183099         0.545667                    0.0490566       0.879245                   -50.8052  46.6095\n",
       "    13       0.702247                    0.117202           0.294443  1.29503            0.109589         0.482                       0.0301887       0.909434                   -70.5557  29.5034\n",
       "    14       0.799157                    0.0923965          0.311512  1.17577            0.115942         0.43761                     0.0301887       0.939623                   -68.8488  17.5767\n",
       "    15       0.898876                    0.0763351          0.454106  1.09571            0.169014         0.407813                    0.045283        0.984906                   -54.5894  9.57075\n",
       "    16       1                           0.0101444          0.149266  1                  0.0555556        0.372191                    0.0150943       1                          -85.0734  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iteration</b></td>\n",
       "<td><b>negative_log_likelihood</b></td>\n",
       "<td><b>objective</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:08</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>469.9993772</td>\n",
       "<td>0.6601115</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:08</td>\n",
       "<td> 0.031 sec</td>\n",
       "<td>1</td>\n",
       "<td>319.4025691</td>\n",
       "<td>0.4494054</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:08</td>\n",
       "<td> 0.033 sec</td>\n",
       "<td>2</td>\n",
       "<td>311.1902527</td>\n",
       "<td>0.4382131</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:08</td>\n",
       "<td> 0.035 sec</td>\n",
       "<td>3</td>\n",
       "<td>310.8603386</td>\n",
       "<td>0.4378358</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:08</td>\n",
       "<td> 0.037 sec</td>\n",
       "<td>4</td>\n",
       "<td>310.8571549</td>\n",
       "<td>0.4378367</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    iteration    negative_log_likelihood    objective\n",
       "--  -------------------  ----------  -----------  -------------------------  -----------\n",
       "    2017-06-29 23:36:08  0.000 sec   0            469.999                    0.660111\n",
       "    2017-06-29 23:36:08  0.031 sec   1            319.403                    0.449405\n",
       "    2017-06-29 23:36:08  0.033 sec   2            311.19                     0.438213\n",
       "    2017-06-29 23:36:08  0.035 sec   3            310.86                     0.437836\n",
       "    2017-06-29 23:36:08  0.037 sec   4            310.857                    0.437837"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on training dataset\n",
    "glm_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.14604424271085578\n",
      "RMSE: 0.38215735333872064\n",
      "LogLoss: 0.46125694368411685\n",
      "Null degrees of freedom: 178\n",
      "Residual degrees of freedom: 167\n",
      "Null deviance: 247.17154578244123\n",
      "Residual deviance: 165.12998583891383\n",
      "AIC: 189.12998583891383\n",
      "AUC: 0.8584797555385791\n",
      "Gini: 0.7169595110771583\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.6037543251670895: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>96.0</td>\n",
       "<td>6.0</td>\n",
       "<td>0.0588</td>\n",
       "<td> (6.0/102.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>24.0</td>\n",
       "<td>53.0</td>\n",
       "<td>0.3117</td>\n",
       "<td> (24.0/77.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>120.0</td>\n",
       "<td>59.0</td>\n",
       "<td>0.1676</td>\n",
       "<td> (30.0/179.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      96   6    0.0588   (6.0/102.0)\n",
       "1      24   53   0.3117   (24.0/77.0)\n",
       "Total  120  59   0.1676   (30.0/179.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.7794118</td>\n",
       "<td>55.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1357796</td>\n",
       "<td>0.8391608</td>\n",
       "<td>113.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.8466454</td>\n",
       "<td>55.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.8324022</td>\n",
       "<td>55.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8170662</td>\n",
       "<td>0.9629630</td>\n",
       "<td>26.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0628163</td>\n",
       "<td>1.0</td>\n",
       "<td>153.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9715229</td>\n",
       "<td>0.9901961</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.6630044</td>\n",
       "<td>55.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4656981</td>\n",
       "<td>0.7532468</td>\n",
       "<td>73.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.6037543</td>\n",
       "<td>0.8147441</td>\n",
       "<td>55.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.603754     0.779412  55\n",
       "max f2                       0.13578      0.839161  113\n",
       "max f0point5                 0.603754     0.846645  55\n",
       "max accuracy                 0.603754     0.832402  55\n",
       "max precision                0.817066     0.962963  26\n",
       "max recall                   0.0628163    1         153\n",
       "max specificity              0.971523     0.990196  0\n",
       "max absolute_mcc             0.603754     0.663004  55\n",
       "max min_per_class_accuracy   0.465698     0.753247  73\n",
       "max mean_per_class_accuracy  0.603754     0.814744  55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 43.02 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0111732</td>\n",
       "<td>0.9640618</td>\n",
       "<td>1.1623377</td>\n",
       "<td>1.1623377</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.0129870</td>\n",
       "<td>16.2337662</td>\n",
       "<td>16.2337662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0223464</td>\n",
       "<td>0.9548586</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.7435065</td>\n",
       "<td>1.0</td>\n",
       "<td>0.75</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0389610</td>\n",
       "<td>132.4675325</td>\n",
       "<td>74.3506494</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9467517</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.9372294</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8333333</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0649351</td>\n",
       "<td>132.4675325</td>\n",
       "<td>93.7229437</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0446927</td>\n",
       "<td>0.9327764</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.0340909</td>\n",
       "<td>1.0</td>\n",
       "<td>0.875</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0909091</td>\n",
       "<td>132.4675325</td>\n",
       "<td>103.4090909</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0502793</td>\n",
       "<td>0.9178485</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.0663781</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1038961</td>\n",
       "<td>132.4675325</td>\n",
       "<td>106.6378066</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1005587</td>\n",
       "<td>0.8898986</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.1955267</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.2207792</td>\n",
       "<td>132.4675325</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1508380</td>\n",
       "<td>0.8139070</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.2385762</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9629630</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.3376623</td>\n",
       "<td>132.4675325</td>\n",
       "<td>123.8576239</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2011173</td>\n",
       "<td>0.7735290</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1955267</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.4415584</td>\n",
       "<td>106.6378066</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3016760</td>\n",
       "<td>0.6387205</td>\n",
       "<td>1.8080808</td>\n",
       "<td>2.0663781</td>\n",
       "<td>0.7777778</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.1818182</td>\n",
       "<td>0.6233766</td>\n",
       "<td>80.8080808</td>\n",
       "<td>106.6378066</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4022346</td>\n",
       "<td>0.4916867</td>\n",
       "<td>1.0331890</td>\n",
       "<td>1.8080808</td>\n",
       "<td>0.4444444</td>\n",
       "<td>0.7777778</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.7272727</td>\n",
       "<td>3.3189033</td>\n",
       "<td>80.8080808</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5027933</td>\n",
       "<td>0.3361618</td>\n",
       "<td>0.7748918</td>\n",
       "<td>1.6014430</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.6888889</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.8051948</td>\n",
       "<td>-22.5108225</td>\n",
       "<td>60.1443001</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5977654</td>\n",
       "<td>0.2088974</td>\n",
       "<td>0.8204736</td>\n",
       "<td>1.4773638</td>\n",
       "<td>0.3529412</td>\n",
       "<td>0.6355140</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.8831169</td>\n",
       "<td>-17.9526356</td>\n",
       "<td>47.7363758</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6983240</td>\n",
       "<td>0.1276819</td>\n",
       "<td>0.5165945</td>\n",
       "<td>1.3390130</td>\n",
       "<td>0.2222222</td>\n",
       "<td>0.576</td>\n",
       "<td>0.0519481</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-48.3405483</td>\n",
       "<td>33.9012987</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7988827</td>\n",
       "<td>0.1089369</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1704659</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5034965</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-100.0</td>\n",
       "<td>17.0465898</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8994413</td>\n",
       "<td>0.0820554</td>\n",
       "<td>0.3874459</td>\n",
       "<td>1.0829233</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.4658385</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.9740260</td>\n",
       "<td>-61.2554113</td>\n",
       "<td>8.2923288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0302597</td>\n",
       "<td>0.2582973</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1111111</td>\n",
       "<td>0.4301676</td>\n",
       "<td>0.0259740</td>\n",
       "<td>1.0</td>\n",
       "<td>-74.1702742</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0111732                   0.964062           1.16234   1.16234            0.5              0.5                         0.012987        0.012987                   16.2338   16.2338\n",
       "    2        0.0223464                   0.954859           2.32468   1.74351            1                0.75                        0.025974        0.038961                   132.468   74.3506\n",
       "    3        0.0335196                   0.946752           2.32468   1.93723            1                0.833333                    0.025974        0.0649351                  132.468   93.7229\n",
       "    4        0.0446927                   0.932776           2.32468   2.03409            1                0.875                       0.025974        0.0909091                  132.468   103.409\n",
       "    5        0.0502793                   0.917848           2.32468   2.06638            1                0.888889                    0.012987        0.103896                   132.468   106.638\n",
       "    6        0.100559                    0.889899           2.32468   2.19553            1                0.944444                    0.116883        0.220779                   132.468   119.553\n",
       "    7        0.150838                    0.813907           2.32468   2.23858            1                0.962963                    0.116883        0.337662                   132.468   123.858\n",
       "    8        0.201117                    0.773529           2.06638   2.19553            0.888889         0.944444                    0.103896        0.441558                   106.638   119.553\n",
       "    9        0.301676                    0.638721           1.80808   2.06638            0.777778         0.888889                    0.181818        0.623377                   80.8081   106.638\n",
       "    10       0.402235                    0.491687           1.03319   1.80808            0.444444         0.777778                    0.103896        0.727273                   3.3189    80.8081\n",
       "    11       0.502793                    0.336162           0.774892  1.60144            0.333333         0.688889                    0.0779221       0.805195                   -22.5108  60.1443\n",
       "    12       0.597765                    0.208897           0.820474  1.47736            0.352941         0.635514                    0.0779221       0.883117                   -17.9526  47.7364\n",
       "    13       0.698324                    0.127682           0.516595  1.33901            0.222222         0.576                       0.0519481       0.935065                   -48.3405  33.9013\n",
       "    14       0.798883                    0.108937           0         1.17047            0                0.503497                    0               0.935065                   -100      17.0466\n",
       "    15       0.899441                    0.0820554          0.387446  1.08292            0.166667         0.465839                    0.038961        0.974026                   -61.2554  8.29233\n",
       "    16       1                           0.0302597          0.258297  1                  0.111111         0.430168                    0.025974        1                          -74.1703  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on test dataset\n",
    "glm_default.model_performance(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "\n",
    "## Distributed Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a Distributed Random Forest (DRF) model with default settings\n",
    "\n",
    "# Import the function for DRF\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "\n",
    "# Set up DRF for regression\n",
    "# Add a seed for reproducibility\n",
    "drf_default = H2ORandomForestEstimator(model_id = 'drf_default', seed = 1234)\n",
    "\n",
    "# Use .train() to build the model\n",
    "drf_default.train(x = features, \n",
    "                  y = 'Survived', \n",
    "                  training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2ORandomForestEstimator :  Distributed Random Forest\n",
      "Model Key:  drf_default\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.1369317850631112\n",
      "RMSE: 0.37004295029511264\n",
      "LogLoss: 0.4610865952257547\n",
      "Mean Per-Class Error: 0.191427124224389\n",
      "AUC: 0.8571567261829387\n",
      "Gini: 0.7143134523658774\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5226372166683799: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>399.0</td>\n",
       "<td>48.0</td>\n",
       "<td>0.1074</td>\n",
       "<td> (48.0/447.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>73.0</td>\n",
       "<td>192.0</td>\n",
       "<td>0.2755</td>\n",
       "<td> (73.0/265.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>472.0</td>\n",
       "<td>240.0</td>\n",
       "<td>0.1699</td>\n",
       "<td> (121.0/712.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      399  48   0.1074   (48.0/447.0)\n",
       "1      73   192  0.2755   (73.0/265.0)\n",
       "Total  472  240  0.1699   (121.0/712.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5226372</td>\n",
       "<td>0.7603960</td>\n",
       "<td>149.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2209992</td>\n",
       "<td>0.7898399</td>\n",
       "<td>247.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6807123</td>\n",
       "<td>0.8059701</td>\n",
       "<td>108.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5369729</td>\n",
       "<td>0.8300562</td>\n",
       "<td>145.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0033102</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5226372</td>\n",
       "<td>0.6310849</td>\n",
       "<td>149.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3439932</td>\n",
       "<td>0.7829978</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5226372</td>\n",
       "<td>0.8085729</td>\n",
       "<td>149.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.522637     0.760396  149\n",
       "max f2                       0.220999     0.78984   247\n",
       "max f0point5                 0.680712     0.80597   108\n",
       "max accuracy                 0.536973     0.830056  145\n",
       "max precision                1            1         0\n",
       "max recall                   0.00331015   1         398\n",
       "max specificity              1            1         0\n",
       "max absolute_mcc             0.522637     0.631085  149\n",
       "max min_per_class_accuracy   0.343993     0.782998  198\n",
       "max mean_per_class_accuracy  0.522637     0.808573  149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 37.22 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0449438</td>\n",
       "<td>0.9995745</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1207547</td>\n",
       "<td>0.1207547</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0575843</td>\n",
       "<td>0.9981459</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0339623</td>\n",
       "<td>0.1547170</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.1011236</td>\n",
       "<td>0.9669435</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1169811</td>\n",
       "<td>0.2716981</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.1502809</td>\n",
       "<td>0.9195929</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1320755</td>\n",
       "<td>0.4037736</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.2008427</td>\n",
       "<td>0.8723681</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1358491</td>\n",
       "<td>0.5396226</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.3005618</td>\n",
       "<td>0.6495294</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2679245</td>\n",
       "<td>0.8075472</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.4002809</td>\n",
       "<td>0.3332100</td>\n",
       "<td>1.7028966</td>\n",
       "<td>2.4416816</td>\n",
       "<td>0.6338028</td>\n",
       "<td>0.9087719</td>\n",
       "<td>0.1698113</td>\n",
       "<td>0.9773585</td>\n",
       "<td>70.2896625</td>\n",
       "<td>144.1681562</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1736858</td>\n",
       "<td>0.1513686</td>\n",
       "<td>1.9849057</td>\n",
       "<td>0.0563380</td>\n",
       "<td>0.7387640</td>\n",
       "<td>0.0150943</td>\n",
       "<td>0.9924528</td>\n",
       "<td>-84.8631411</td>\n",
       "<td>98.4905660</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.6095506</td>\n",
       "<td>0.1077212</td>\n",
       "<td>0.0688921</td>\n",
       "<td>1.6405530</td>\n",
       "<td>0.0256410</td>\n",
       "<td>0.6105991</td>\n",
       "<td>0.0075472</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.1107886</td>\n",
       "<td>64.0552995</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.6994382</td>\n",
       "<td>0.0658923</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4297189</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5321285</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.9718876</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.7991573</td>\n",
       "<td>0.0438331</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2513181</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4657293</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.1318102</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.8988764</td>\n",
       "<td>0.0221624</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1125</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4140625</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.2500000</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3721910</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0449438                   0.999574           2.68679    2.68679            1                1                           0.120755        0.120755                   168.679   168.679\n",
       "    2        0.0575843                   0.998146           2.68679    2.68679            1                1                           0.0339623       0.154717                   168.679   168.679\n",
       "    3        0.101124                    0.966944           2.68679    2.68679            1                1                           0.116981        0.271698                   168.679   168.679\n",
       "    4        0.150281                    0.919593           2.68679    2.68679            1                1                           0.132075        0.403774                   168.679   168.679\n",
       "    5        0.200843                    0.872368           2.68679    2.68679            1                1                           0.135849        0.539623                   168.679   168.679\n",
       "    6        0.300562                    0.649529           2.68679    2.68679            1                1                           0.267925        0.807547                   168.679   168.679\n",
       "    7        0.400281                    0.33321            1.7029     2.44168            0.633803         0.908772                    0.169811        0.977358                   70.2897   144.168\n",
       "    8        0.5                         0.173686           0.151369   1.98491            0.056338         0.738764                    0.0150943       0.992453                   -84.8631  98.4906\n",
       "    9        0.609551                    0.107721           0.0688921  1.64055            0.025641         0.610599                    0.00754717      1                          -93.1108  64.0553\n",
       "    10       0.699438                    0.0658923          0          1.42972            0                0.532129                    0               1                          -100      42.9719\n",
       "    11       0.799157                    0.0438331          0          1.25132            0                0.465729                    0               1                          -100      25.1318\n",
       "    12       0.898876                    0.0221624          0          1.1125             0                0.414062                    0               1                          -100      11.25\n",
       "    13       1                           0                  0          1                  0                0.372191                    0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:09</td>\n",
       "<td> 0.014 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:09</td>\n",
       "<td> 0.132 sec</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4291874</td>\n",
       "<td>5.9978498</td>\n",
       "<td>0.7669266</td>\n",
       "<td>2.4437017</td>\n",
       "<td>0.1839080</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:09</td>\n",
       "<td> 0.171 sec</td>\n",
       "<td>2.0</td>\n",
       "<td>0.4322233</td>\n",
       "<td>5.6811387</td>\n",
       "<td>0.7908389</td>\n",
       "<td>2.6141764</td>\n",
       "<td>0.1961259</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:09</td>\n",
       "<td> 0.193 sec</td>\n",
       "<td>3.0</td>\n",
       "<td>0.4428774</td>\n",
       "<td>5.7965230</td>\n",
       "<td>0.7724040</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.2011385</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:09</td>\n",
       "<td> 0.218 sec</td>\n",
       "<td>4.0</td>\n",
       "<td>0.4366525</td>\n",
       "<td>5.0403270</td>\n",
       "<td>0.7734675</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.2003367</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:10</td>\n",
       "<td> 1.150 sec</td>\n",
       "<td>46.0</td>\n",
       "<td>0.3706536</td>\n",
       "<td>0.4636126</td>\n",
       "<td>0.8561606</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1853933</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:10</td>\n",
       "<td> 1.170 sec</td>\n",
       "<td>47.0</td>\n",
       "<td>0.3717892</td>\n",
       "<td>0.4663608</td>\n",
       "<td>0.8544004</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1727528</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:10</td>\n",
       "<td> 1.197 sec</td>\n",
       "<td>48.0</td>\n",
       "<td>0.3715805</td>\n",
       "<td>0.4660573</td>\n",
       "<td>0.8550293</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1769663</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:10</td>\n",
       "<td> 1.220 sec</td>\n",
       "<td>49.0</td>\n",
       "<td>0.3708112</td>\n",
       "<td>0.4623615</td>\n",
       "<td>0.8564476</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1769663</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:10</td>\n",
       "<td> 1.245 sec</td>\n",
       "<td>50.0</td>\n",
       "<td>0.3700430</td>\n",
       "<td>0.4610866</td>\n",
       "<td>0.8571567</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1699438</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    number_of_trees    training_rmse        training_logloss     training_auc        training_lift       training_classification_error\n",
       "---  -------------------  ----------  -----------------  -------------------  -------------------  ------------------  ------------------  -------------------------------\n",
       "     2017-06-29 23:36:09  0.014 sec   0.0                nan                  nan                  nan                 nan                 nan\n",
       "     2017-06-29 23:36:09  0.132 sec   1.0                0.4291873574382064   5.997849846746698    0.7669265756985055  2.4437017070979334  0.1839080459770115\n",
       "     2017-06-29 23:36:09  0.171 sec   2.0                0.4322233040379346   5.6811386657219565   0.7908389089037571  2.6141764405915353  0.19612590799031476\n",
       "     2017-06-29 23:36:09  0.193 sec   3.0                0.4428773880788786   5.796522989560667    0.7724039829302988  2.6867924528301885  0.20113851992409867\n",
       "     2017-06-29 23:36:09  0.218 sec   4.0                0.4366524615399937   5.0403270106286655   0.7734675093575504  2.6867924528301885  0.20033670033670034\n",
       "---  ---                  ---         ---                ---                  ---                  ---                 ---                 ---\n",
       "     2017-06-29 23:36:10  1.150 sec   46.0               0.37065363158888787  0.4636125625060299   0.8561605673040396  2.6867924528301885  0.1853932584269663\n",
       "     2017-06-29 23:36:10  1.170 sec   47.0               0.37178920237348906  0.4663607782594671   0.8544004052171711  2.6867924528301885  0.17275280898876405\n",
       "     2017-06-29 23:36:10  1.197 sec   48.0               0.3715805000920975   0.4660572597799091   0.8550293360347812  2.6867924528301885  0.17696629213483145\n",
       "     2017-06-29 23:36:10  1.220 sec   49.0               0.37081122145053086  0.46236148166886804  0.8564475961335528  2.6867924528301885  0.17696629213483145\n",
       "     2017-06-29 23:36:10  1.245 sec   50.0               0.37004295029511264  0.4610865952257547   0.8571567261829387  2.6867924528301885  0.1699438202247191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>Sex</td>\n",
       "<td>1513.9517822</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3113656</td></tr>\n",
       "<tr><td>Age</td>\n",
       "<td>1163.5933838</td>\n",
       "<td>0.7685802</td>\n",
       "<td>0.2393095</td></tr>\n",
       "<tr><td>Fare</td>\n",
       "<td>1132.6246338</td>\n",
       "<td>0.7481246</td>\n",
       "<td>0.2329403</td></tr>\n",
       "<tr><td>Pclass</td>\n",
       "<td>451.8060913</td>\n",
       "<td>0.2984283</td>\n",
       "<td>0.0929203</td></tr>\n",
       "<tr><td>SibSp</td>\n",
       "<td>263.0476379</td>\n",
       "<td>0.1737490</td>\n",
       "<td>0.0540995</td></tr>\n",
       "<tr><td>Embarked</td>\n",
       "<td>168.9029541</td>\n",
       "<td>0.1115643</td>\n",
       "<td>0.0347373</td></tr>\n",
       "<tr><td>Parch</td>\n",
       "<td>168.3693390</td>\n",
       "<td>0.1112118</td>\n",
       "<td>0.0346275</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance    percentage\n",
       "----------  ---------------------  -------------------  ------------\n",
       "Sex         1513.95                1                    0.311366\n",
       "Age         1163.59                0.76858              0.239309\n",
       "Fare        1132.62                0.748125             0.23294\n",
       "Pclass      451.806                0.298428             0.0929203\n",
       "SibSp       263.048                0.173749             0.0540995\n",
       "Embarked    168.903                0.111564             0.0347373\n",
       "Parch       168.369                0.111212             0.0346275"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the DRF model summary\n",
    "drf_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.1274604300255528\n",
      "RMSE: 0.3570160080802439\n",
      "LogLoss: 0.41184768288178214\n",
      "Mean Per-Class Error: 0.1690858161446397\n",
      "AUC: 0.8883371530430353\n",
      "Gini: 0.7766743060860706\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5416861137747764: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>94.0</td>\n",
       "<td>8.0</td>\n",
       "<td>0.0784</td>\n",
       "<td> (8.0/102.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>20.0</td>\n",
       "<td>57.0</td>\n",
       "<td>0.2597</td>\n",
       "<td> (20.0/77.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>114.0</td>\n",
       "<td>65.0</td>\n",
       "<td>0.1564</td>\n",
       "<td> (28.0/179.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      94   8    0.0784   (8.0/102.0)\n",
       "1      20   57   0.2597   (20.0/77.0)\n",
       "Total  114  65   0.1564   (28.0/179.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5416861</td>\n",
       "<td>0.8028169</td>\n",
       "<td>56.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1513190</td>\n",
       "<td>0.8488372</td>\n",
       "<td>112.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5672688</td>\n",
       "<td>0.8510638</td>\n",
       "<td>54.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5672688</td>\n",
       "<td>0.8435754</td>\n",
       "<td>54.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0101172</td>\n",
       "<td>1.0</td>\n",
       "<td>153.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5672688</td>\n",
       "<td>0.6828067</td>\n",
       "<td>54.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3550097</td>\n",
       "<td>0.8051948</td>\n",
       "<td>70.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5416861</td>\n",
       "<td>0.8309142</td>\n",
       "<td>56.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.541686     0.802817  56\n",
       "max f2                       0.151319     0.848837  112\n",
       "max f0point5                 0.567269     0.851064  54\n",
       "max accuracy                 0.567269     0.843575  54\n",
       "max precision                1            1         0\n",
       "max recall                   0.0101172    1         153\n",
       "max specificity              1            1         0\n",
       "max absolute_mcc             0.567269     0.682807  54\n",
       "max min_per_class_accuracy   0.35501      0.805195  70\n",
       "max mean_per_class_accuracy  0.541686     0.830914  56"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 43.02 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9995745</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.0779221</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9927745</td>\n",
       "<td>0.0</td>\n",
       "<td>2.3246753</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0779221</td>\n",
       "<td>-100.0</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0446927</td>\n",
       "<td>0.9781327</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.1038961</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0502793</td>\n",
       "<td>0.9691349</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1168831</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.1005587</td>\n",
       "<td>0.9319590</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.2337662</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1564246</td>\n",
       "<td>0.8745745</td>\n",
       "<td>2.0922078</td>\n",
       "<td>2.2416512</td>\n",
       "<td>0.9</td>\n",
       "<td>0.9642857</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.3506494</td>\n",
       "<td>109.2207792</td>\n",
       "<td>124.1651206</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.2067039</td>\n",
       "<td>0.8625339</td>\n",
       "<td>1.8080808</td>\n",
       "<td>2.1361881</td>\n",
       "<td>0.7777778</td>\n",
       "<td>0.9189189</td>\n",
       "<td>0.0909091</td>\n",
       "<td>0.4415584</td>\n",
       "<td>80.8080808</td>\n",
       "<td>113.6188136</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.3016760</td>\n",
       "<td>0.6292381</td>\n",
       "<td>2.0511841</td>\n",
       "<td>2.1094276</td>\n",
       "<td>0.8823529</td>\n",
       "<td>0.9074074</td>\n",
       "<td>0.1948052</td>\n",
       "<td>0.6363636</td>\n",
       "<td>105.1184110</td>\n",
       "<td>110.9427609</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.4022346</td>\n",
       "<td>0.4531885</td>\n",
       "<td>1.2914863</td>\n",
       "<td>1.9049423</td>\n",
       "<td>0.5555556</td>\n",
       "<td>0.8194444</td>\n",
       "<td>0.1298701</td>\n",
       "<td>0.7662338</td>\n",
       "<td>29.1486291</td>\n",
       "<td>90.4942280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.5027933</td>\n",
       "<td>0.2850131</td>\n",
       "<td>0.7748918</td>\n",
       "<td>1.6789322</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.7222222</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.8441558</td>\n",
       "<td>-22.5108225</td>\n",
       "<td>67.8932179</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5977654</td>\n",
       "<td>0.2027503</td>\n",
       "<td>0.4102368</td>\n",
       "<td>1.4773638</td>\n",
       "<td>0.1764706</td>\n",
       "<td>0.6355140</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.8831169</td>\n",
       "<td>-58.9763178</td>\n",
       "<td>47.7363758</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.7262570</td>\n",
       "<td>0.1353284</td>\n",
       "<td>0.5053642</td>\n",
       "<td>1.3053946</td>\n",
       "<td>0.2173913</td>\n",
       "<td>0.5615385</td>\n",
       "<td>0.0649351</td>\n",
       "<td>0.9480519</td>\n",
       "<td>-49.4635799</td>\n",
       "<td>30.5394605</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7988827</td>\n",
       "<td>0.0743745</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1867224</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5104895</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9480519</td>\n",
       "<td>-100.0</td>\n",
       "<td>18.6722369</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8994413</td>\n",
       "<td>0.0400742</td>\n",
       "<td>0.3874459</td>\n",
       "<td>1.0973623</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.4720497</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.9870130</td>\n",
       "<td>-61.2554113</td>\n",
       "<td>9.7362265</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0080119</td>\n",
       "<td>0.1291486</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.4301676</td>\n",
       "<td>0.0129870</td>\n",
       "<td>1.0</td>\n",
       "<td>-87.0851371</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0335196                   0.999574           2.32468   2.32468            1                1                           0.0779221       0.0779221                  132.468   132.468\n",
       "    2        0.0335196                   0.992774           0         2.32468            0                1                           0               0.0779221                  -100      132.468\n",
       "    3        0.0446927                   0.978133           2.32468   2.32468            1                1                           0.025974        0.103896                   132.468   132.468\n",
       "    4        0.0502793                   0.969135           2.32468   2.32468            1                1                           0.012987        0.116883                   132.468   132.468\n",
       "    5        0.100559                    0.931959           2.32468   2.32468            1                1                           0.116883        0.233766                   132.468   132.468\n",
       "    6        0.156425                    0.874574           2.09221   2.24165            0.9              0.964286                    0.116883        0.350649                   109.221   124.165\n",
       "    7        0.206704                    0.862534           1.80808   2.13619            0.777778         0.918919                    0.0909091       0.441558                   80.8081   113.619\n",
       "    8        0.301676                    0.629238           2.05118   2.10943            0.882353         0.907407                    0.194805        0.636364                   105.118   110.943\n",
       "    9        0.402235                    0.453189           1.29149   1.90494            0.555556         0.819444                    0.12987         0.766234                   29.1486   90.4942\n",
       "    10       0.502793                    0.285013           0.774892  1.67893            0.333333         0.722222                    0.0779221       0.844156                   -22.5108  67.8932\n",
       "    11       0.597765                    0.20275            0.410237  1.47736            0.176471         0.635514                    0.038961        0.883117                   -58.9763  47.7364\n",
       "    12       0.726257                    0.135328           0.505364  1.30539            0.217391         0.561538                    0.0649351       0.948052                   -49.4636  30.5395\n",
       "    13       0.798883                    0.0743745          0         1.18672            0                0.51049                     0               0.948052                   -100      18.6722\n",
       "    14       0.899441                    0.0400742          0.387446  1.09736            0.166667         0.47205                     0.038961        0.987013                   -61.2554  9.73623\n",
       "    15       1                           0.0080119          0.129149  1                  0.0555556        0.430168                    0.012987        1                          -87.0851  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on test dataset\n",
    "drf_default.model_performance(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "\n",
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a Gradient Boosting Machines (GBM) model with default settings\n",
    "\n",
    "# Import the function for GBM\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "# Set up GBM for regression\n",
    "# Add a seed for reproducibility\n",
    "gbm_default = H2OGradientBoostingEstimator(model_id = 'gbm_default', seed = 1234)\n",
    "\n",
    "# Use .train() to build the model\n",
    "gbm_default.train(x = features, \n",
    "                  y = 'Survived', \n",
    "                  training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  gbm_default\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.07758897316685097\n",
      "RMSE: 0.2785479728284716\n",
      "LogLoss: 0.2713297688833869\n",
      "Mean Per-Class Error: 0.1047697437845595\n",
      "AUC: 0.9547465282174665\n",
      "Gini: 0.9094930564349331\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.414932842273446: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>417.0</td>\n",
       "<td>30.0</td>\n",
       "<td>0.0671</td>\n",
       "<td> (30.0/447.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>38.0</td>\n",
       "<td>227.0</td>\n",
       "<td>0.1434</td>\n",
       "<td> (38.0/265.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>455.0</td>\n",
       "<td>257.0</td>\n",
       "<td>0.0955</td>\n",
       "<td> (68.0/712.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      417  30   0.0671   (30.0/447.0)\n",
       "1      38   227  0.1434   (38.0/265.0)\n",
       "Total  455  257  0.0955   (68.0/712.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4149328</td>\n",
       "<td>0.8697318</td>\n",
       "<td>182.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.3208260</td>\n",
       "<td>0.8792846</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6537940</td>\n",
       "<td>0.9107981</td>\n",
       "<td>133.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4816206</td>\n",
       "<td>0.9073034</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9901771</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0878633</td>\n",
       "<td>1.0</td>\n",
       "<td>332.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9901771</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4816206</td>\n",
       "<td>0.8000835</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3208260</td>\n",
       "<td>0.8905660</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.3904755</td>\n",
       "<td>0.8952303</td>\n",
       "<td>192.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.414933     0.869732  182\n",
       "max f2                       0.320826     0.879285  202\n",
       "max f0point5                 0.653794     0.910798  133\n",
       "max accuracy                 0.481621     0.907303  167\n",
       "max precision                0.990177     1         0\n",
       "max recall                   0.0878633    1         332\n",
       "max specificity              0.990177     1         0\n",
       "max absolute_mcc             0.481621     0.800083  167\n",
       "max min_per_class_accuracy   0.320826     0.890566  202\n",
       "max mean_per_class_accuracy  0.390476     0.89523   192"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 37.22 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0112360</td>\n",
       "<td>0.9866256</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.0301887</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0210674</td>\n",
       "<td>0.9827783</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0566038</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0308989</td>\n",
       "<td>0.9801608</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0830189</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0407303</td>\n",
       "<td>0.9790456</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1094340</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0505618</td>\n",
       "<td>0.9752566</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1358491</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1011236</td>\n",
       "<td>0.9606552</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1358491</td>\n",
       "<td>0.2716981</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1502809</td>\n",
       "<td>0.9243655</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1320755</td>\n",
       "<td>0.4037736</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2008427</td>\n",
       "<td>0.8073383</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1358491</td>\n",
       "<td>0.5396226</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3005618</td>\n",
       "<td>0.5918481</td>\n",
       "<td>2.1948445</td>\n",
       "<td>2.5235761</td>\n",
       "<td>0.8169014</td>\n",
       "<td>0.9392523</td>\n",
       "<td>0.2188679</td>\n",
       "<td>0.7584906</td>\n",
       "<td>119.4844539</td>\n",
       "<td>152.3576089</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4002809</td>\n",
       "<td>0.3038840</td>\n",
       "<td>1.3244752</td>\n",
       "<td>2.2248527</td>\n",
       "<td>0.4929577</td>\n",
       "<td>0.8280702</td>\n",
       "<td>0.1320755</td>\n",
       "<td>0.8905660</td>\n",
       "<td>32.4475153</td>\n",
       "<td>122.4852698</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1634237</td>\n",
       "<td>0.4162636</td>\n",
       "<td>1.8641509</td>\n",
       "<td>0.1549296</td>\n",
       "<td>0.6938202</td>\n",
       "<td>0.0415094</td>\n",
       "<td>0.9320755</td>\n",
       "<td>-58.3736381</td>\n",
       "<td>86.4150943</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6025281</td>\n",
       "<td>0.1231070</td>\n",
       "<td>0.2944430</td>\n",
       "<td>1.5970445</td>\n",
       "<td>0.1095890</td>\n",
       "<td>0.5944056</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.9622642</td>\n",
       "<td>-70.5556991</td>\n",
       "<td>59.7044465</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6994382</td>\n",
       "<td>0.0977094</td>\n",
       "<td>0.2725731</td>\n",
       "<td>1.4135334</td>\n",
       "<td>0.1014493</td>\n",
       "<td>0.5261044</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.9886792</td>\n",
       "<td>-72.7426853</td>\n",
       "<td>41.3533379</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7991573</td>\n",
       "<td>0.0871370</td>\n",
       "<td>0.1135264</td>\n",
       "<td>1.2513181</td>\n",
       "<td>0.0422535</td>\n",
       "<td>0.4657293</td>\n",
       "<td>0.0113208</td>\n",
       "<td>1.0</td>\n",
       "<td>-88.6473558</td>\n",
       "<td>25.1318102</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8988764</td>\n",
       "<td>0.0657414</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1125</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4140625</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.2500000</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0198764</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3721910</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.011236                    0.986626           2.68679   2.68679            1                1                           0.0301887       0.0301887                  168.679   168.679\n",
       "    2        0.0210674                   0.982778           2.68679   2.68679            1                1                           0.0264151       0.0566038                  168.679   168.679\n",
       "    3        0.0308989                   0.980161           2.68679   2.68679            1                1                           0.0264151       0.0830189                  168.679   168.679\n",
       "    4        0.0407303                   0.979046           2.68679   2.68679            1                1                           0.0264151       0.109434                   168.679   168.679\n",
       "    5        0.0505618                   0.975257           2.68679   2.68679            1                1                           0.0264151       0.135849                   168.679   168.679\n",
       "    6        0.101124                    0.960655           2.68679   2.68679            1                1                           0.135849        0.271698                   168.679   168.679\n",
       "    7        0.150281                    0.924365           2.68679   2.68679            1                1                           0.132075        0.403774                   168.679   168.679\n",
       "    8        0.200843                    0.807338           2.68679   2.68679            1                1                           0.135849        0.539623                   168.679   168.679\n",
       "    9        0.300562                    0.591848           2.19484   2.52358            0.816901         0.939252                    0.218868        0.758491                   119.484   152.358\n",
       "    10       0.400281                    0.303884           1.32448   2.22485            0.492958         0.82807                     0.132075        0.890566                   32.4475   122.485\n",
       "    11       0.5                         0.163424           0.416264  1.86415            0.15493          0.69382                     0.0415094       0.932075                   -58.3736  86.4151\n",
       "    12       0.602528                    0.123107           0.294443  1.59704            0.109589         0.594406                    0.0301887       0.962264                   -70.5557  59.7044\n",
       "    13       0.699438                    0.0977094          0.272573  1.41353            0.101449         0.526104                    0.0264151       0.988679                   -72.7427  41.3533\n",
       "    14       0.799157                    0.087137           0.113526  1.25132            0.0422535        0.465729                    0.0113208       1                          -88.6474  25.1318\n",
       "    15       0.898876                    0.0657414          0         1.1125             0                0.414062                    0               1                          -100      11.25\n",
       "    16       1                           0.0198764          0         1                  0                0.372191                    0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.002 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4833889</td>\n",
       "<td>0.6601115</td>\n",
       "<td>0.5</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6278090</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.054 sec</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4598999</td>\n",
       "<td>0.6132126</td>\n",
       "<td>0.8876493</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1671348</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.068 sec</td>\n",
       "<td>2.0</td>\n",
       "<td>0.4400875</td>\n",
       "<td>0.5757196</td>\n",
       "<td>0.8964248</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1755618</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.079 sec</td>\n",
       "<td>3.0</td>\n",
       "<td>0.4235059</td>\n",
       "<td>0.5452385</td>\n",
       "<td>0.8968469</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1685393</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.089 sec</td>\n",
       "<td>4.0</td>\n",
       "<td>0.4093690</td>\n",
       "<td>0.5196299</td>\n",
       "<td>0.9012874</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1601124</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.704 sec</td>\n",
       "<td>46.0</td>\n",
       "<td>0.2832313</td>\n",
       "<td>0.2788065</td>\n",
       "<td>0.9512642</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1011236</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.718 sec</td>\n",
       "<td>47.0</td>\n",
       "<td>0.2818163</td>\n",
       "<td>0.2767664</td>\n",
       "<td>0.9518551</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1025281</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.767 sec</td>\n",
       "<td>48.0</td>\n",
       "<td>0.2812117</td>\n",
       "<td>0.2757954</td>\n",
       "<td>0.9523954</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1011236</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.792 sec</td>\n",
       "<td>49.0</td>\n",
       "<td>0.2798017</td>\n",
       "<td>0.2734512</td>\n",
       "<td>0.9533620</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.0997191</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:11</td>\n",
       "<td> 0.811 sec</td>\n",
       "<td>50.0</td>\n",
       "<td>0.2785480</td>\n",
       "<td>0.2713298</td>\n",
       "<td>0.9547465</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.0955056</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    number_of_trees    training_rmse        training_logloss    training_auc        training_lift       training_classification_error\n",
       "---  -------------------  ----------  -----------------  -------------------  ------------------  ------------------  ------------------  -------------------------------\n",
       "     2017-06-29 23:36:11  0.002 sec   0.0                0.4833889349076107   0.6601114848970648  0.5                 1.0                 0.6278089887640449\n",
       "     2017-06-29 23:36:11  0.054 sec   1.0                0.45989986689221923  0.6132125941626966  0.8876493183065299  2.6867924528301885  0.16713483146067415\n",
       "     2017-06-29 23:36:11  0.068 sec   2.0                0.4400875187902207   0.5757195983670309  0.8964248026676798  2.6867924528301885  0.175561797752809\n",
       "     2017-06-29 23:36:11  0.079 sec   3.0                0.4235058566314087   0.5452385076204875  0.8968469038875522  2.6867924528301885  0.16853932584269662\n",
       "     2017-06-29 23:36:11  0.089 sec   4.0                0.40936898295634916  0.5196299221482236  0.9012874087206112  2.6867924528301885  0.1601123595505618\n",
       "---  ---                  ---         ---                ---                  ---                 ---                 ---                 ---\n",
       "     2017-06-29 23:36:11  0.704 sec   46.0               0.2832313313042898   0.2788065370744706  0.9512641931535182  2.6867924528301885  0.10112359550561797\n",
       "     2017-06-29 23:36:11  0.718 sec   47.0               0.28181627994627284  0.2767663993022549  0.9518551348613398  2.6867924528301885  0.10252808988764045\n",
       "     2017-06-29 23:36:11  0.767 sec   48.0               0.2812116794952194   0.2757954350818175  0.9523954244227765  2.6867924528301885  0.10112359550561797\n",
       "     2017-06-29 23:36:11  0.792 sec   49.0               0.2798016840952948   0.2734512350771905  0.9533620362162846  2.6867924528301885  0.0997191011235955\n",
       "     2017-06-29 23:36:11  0.811 sec   50.0               0.2785479728284716   0.2713297688833869  0.9547465282174665  2.6867924528301885  0.09550561797752809"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>Sex</td>\n",
       "<td>269.9861755</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4711521</td></tr>\n",
       "<tr><td>Age</td>\n",
       "<td>103.5973969</td>\n",
       "<td>0.3837137</td>\n",
       "<td>0.1807875</td></tr>\n",
       "<tr><td>Pclass</td>\n",
       "<td>80.2009659</td>\n",
       "<td>0.2970558</td>\n",
       "<td>0.1399585</td></tr>\n",
       "<tr><td>Fare</td>\n",
       "<td>77.6247101</td>\n",
       "<td>0.2875136</td>\n",
       "<td>0.1354627</td></tr>\n",
       "<tr><td>SibSp</td>\n",
       "<td>29.2148132</td>\n",
       "<td>0.1082086</td>\n",
       "<td>0.0509827</td></tr>\n",
       "<tr><td>Embarked</td>\n",
       "<td>9.1480532</td>\n",
       "<td>0.0338834</td>\n",
       "<td>0.0159642</td></tr>\n",
       "<tr><td>Parch</td>\n",
       "<td>3.2619276</td>\n",
       "<td>0.0120818</td>\n",
       "<td>0.0056924</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance    percentage\n",
       "----------  ---------------------  -------------------  ------------\n",
       "Sex         269.986                1                    0.471152\n",
       "Age         103.597                0.383714             0.180788\n",
       "Pclass      80.201                 0.297056             0.139958\n",
       "Fare        77.6247                0.287514             0.135463\n",
       "SibSp       29.2148                0.108209             0.0509827\n",
       "Embarked    9.14805                0.0338834            0.0159642\n",
       "Parch       3.26193                0.0120818            0.00569238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the GBM model summary\n",
    "gbm_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.12599930735750908\n",
      "RMSE: 0.3549638113350558\n",
      "LogLoss: 0.40967742175755323\n",
      "Mean Per-Class Error: 0.16603004838298951\n",
      "AUC: 0.8834351922587216\n",
      "Gini: 0.7668703845174432\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.40893590243834643: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>88.0</td>\n",
       "<td>14.0</td>\n",
       "<td>0.1373</td>\n",
       "<td> (14.0/102.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>15.0</td>\n",
       "<td>62.0</td>\n",
       "<td>0.1948</td>\n",
       "<td> (15.0/77.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>103.0</td>\n",
       "<td>76.0</td>\n",
       "<td>0.162</td>\n",
       "<td> (29.0/179.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      88   14   0.1373   (14.0/102.0)\n",
       "1      15   62   0.1948   (15.0/77.0)\n",
       "Total  103  76   0.162    (29.0/179.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4089359</td>\n",
       "<td>0.8104575</td>\n",
       "<td>68.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1739562</td>\n",
       "<td>0.8658537</td>\n",
       "<td>90.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6215871</td>\n",
       "<td>0.8626198</td>\n",
       "<td>51.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6215871</td>\n",
       "<td>0.8435754</td>\n",
       "<td>51.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9874823</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0607561</td>\n",
       "<td>1.0</td>\n",
       "<td>142.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9874823</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.6215871</td>\n",
       "<td>0.6870088</td>\n",
       "<td>51.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2946931</td>\n",
       "<td>0.8181818</td>\n",
       "<td>73.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4089359</td>\n",
       "<td>0.8339700</td>\n",
       "<td>68.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.408936     0.810458  68\n",
       "max f2                       0.173956     0.865854  90\n",
       "max f0point5                 0.621587     0.86262   51\n",
       "max accuracy                 0.621587     0.843575  51\n",
       "max precision                0.987482     1         0\n",
       "max recall                   0.0607561    1         142\n",
       "max specificity              0.987482     1         0\n",
       "max absolute_mcc             0.621587     0.687009  51\n",
       "max min_per_class_accuracy   0.294693     0.818182  73\n",
       "max mean_per_class_accuracy  0.408936     0.83397   68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 43.02 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0111732</td>\n",
       "<td>0.9855215</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0259740</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0223464</td>\n",
       "<td>0.9815861</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0519481</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9785022</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0779221</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0446927</td>\n",
       "<td>0.9766887</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.1038961</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0502793</td>\n",
       "<td>0.9709074</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1168831</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1005587</td>\n",
       "<td>0.9594569</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1955267</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.2207792</td>\n",
       "<td>106.6378066</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1508380</td>\n",
       "<td>0.9260351</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1524772</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9259259</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.3246753</td>\n",
       "<td>106.6378066</td>\n",
       "<td>115.2477152</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2011173</td>\n",
       "<td>0.8804140</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1309524</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9166667</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.4285714</td>\n",
       "<td>106.6378066</td>\n",
       "<td>113.0952381</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3016760</td>\n",
       "<td>0.6805693</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1094276</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9074074</td>\n",
       "<td>0.2077922</td>\n",
       "<td>0.6363636</td>\n",
       "<td>106.6378066</td>\n",
       "<td>110.9427609</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4022346</td>\n",
       "<td>0.4574617</td>\n",
       "<td>1.2914863</td>\n",
       "<td>1.9049423</td>\n",
       "<td>0.5555556</td>\n",
       "<td>0.8194444</td>\n",
       "<td>0.1298701</td>\n",
       "<td>0.7662338</td>\n",
       "<td>29.1486291</td>\n",
       "<td>90.4942280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5027933</td>\n",
       "<td>0.2239804</td>\n",
       "<td>0.7748918</td>\n",
       "<td>1.6789322</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.7222222</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.8441558</td>\n",
       "<td>-22.5108225</td>\n",
       "<td>67.8932179</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6089385</td>\n",
       "<td>0.1669759</td>\n",
       "<td>0.7341080</td>\n",
       "<td>1.5142381</td>\n",
       "<td>0.3157895</td>\n",
       "<td>0.6513761</td>\n",
       "<td>0.0779221</td>\n",
       "<td>0.9220779</td>\n",
       "<td>-26.5892003</td>\n",
       "<td>51.4238056</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6983240</td>\n",
       "<td>0.1096594</td>\n",
       "<td>0.1452922</td>\n",
       "<td>1.3390130</td>\n",
       "<td>0.0625</td>\n",
       "<td>0.576</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-85.4707792</td>\n",
       "<td>33.9012987</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7988827</td>\n",
       "<td>0.0897154</td>\n",
       "<td>0.1291486</td>\n",
       "<td>1.1867224</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.5104895</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.9480519</td>\n",
       "<td>-87.0851371</td>\n",
       "<td>18.6722369</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8994413</td>\n",
       "<td>0.0758639</td>\n",
       "<td>0.1291486</td>\n",
       "<td>1.0684843</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.4596273</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.9610390</td>\n",
       "<td>-87.0851371</td>\n",
       "<td>6.8484311</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0372517</td>\n",
       "<td>0.3874459</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.4301676</td>\n",
       "<td>0.0389610</td>\n",
       "<td>1.0</td>\n",
       "<td>-61.2554113</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0111732                   0.985522           2.32468   2.32468            1                1                           0.025974        0.025974                   132.468   132.468\n",
       "    2        0.0223464                   0.981586           2.32468   2.32468            1                1                           0.025974        0.0519481                  132.468   132.468\n",
       "    3        0.0335196                   0.978502           2.32468   2.32468            1                1                           0.025974        0.0779221                  132.468   132.468\n",
       "    4        0.0446927                   0.976689           2.32468   2.32468            1                1                           0.025974        0.103896                   132.468   132.468\n",
       "    5        0.0502793                   0.970907           2.32468   2.32468            1                1                           0.012987        0.116883                   132.468   132.468\n",
       "    6        0.100559                    0.959457           2.06638   2.19553            0.888889         0.944444                    0.103896        0.220779                   106.638   119.553\n",
       "    7        0.150838                    0.926035           2.06638   2.15248            0.888889         0.925926                    0.103896        0.324675                   106.638   115.248\n",
       "    8        0.201117                    0.880414           2.06638   2.13095            0.888889         0.916667                    0.103896        0.428571                   106.638   113.095\n",
       "    9        0.301676                    0.680569           2.06638   2.10943            0.888889         0.907407                    0.207792        0.636364                   106.638   110.943\n",
       "    10       0.402235                    0.457462           1.29149   1.90494            0.555556         0.819444                    0.12987         0.766234                   29.1486   90.4942\n",
       "    11       0.502793                    0.22398            0.774892  1.67893            0.333333         0.722222                    0.0779221       0.844156                   -22.5108  67.8932\n",
       "    12       0.608939                    0.166976           0.734108  1.51424            0.315789         0.651376                    0.0779221       0.922078                   -26.5892  51.4238\n",
       "    13       0.698324                    0.109659           0.145292  1.33901            0.0625           0.576                       0.012987        0.935065                   -85.4708  33.9013\n",
       "    14       0.798883                    0.0897154          0.129149  1.18672            0.0555556        0.51049                     0.012987        0.948052                   -87.0851  18.6722\n",
       "    15       0.899441                    0.0758639          0.129149  1.06848            0.0555556        0.459627                    0.012987        0.961039                   -87.0851  6.84843\n",
       "    16       1                           0.0372517          0.387446  1                  0.166667         0.430168                    0.038961        1                          -61.2554  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on test dataset\n",
    "gbm_default.model_performance(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## H2O Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Model Build progress: |██████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# Build a Deep Learning (Deep Neural Networks, DNN) model with default settings\n",
    "\n",
    "# Import the function for DNN\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "# Set up DNN for regression\n",
    "dnn_default = H2ODeepLearningEstimator(model_id = 'dnn_default')\n",
    "\n",
    "# (not run) Change 'reproducible' to True if you want to reproduce the results\n",
    "# The model will be built using a single thread (could be very slow)\n",
    "# dnn_default = H2ODeepLearningEstimator(model_id = 'dnn_default', reproducible = True)\n",
    "\n",
    "# Use .train() to build the model\n",
    "dnn_default.train(x = features, \n",
    "                  y = 'Survived', \n",
    "                  training_frame = titanic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2ODeepLearningEstimator :  Deep Learning\n",
      "Model Key:  dnn_default\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.14196070454042292\n",
      "RMSE: 0.37677673035953657\n",
      "LogLoss: 0.5042119395055878\n",
      "Mean Per-Class Error: 0.1785023848718923\n",
      "AUC: 0.8779030011396732\n",
      "Gini: 0.7558060022793465\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2334151945054888: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>421.0</td>\n",
       "<td>26.0</td>\n",
       "<td>0.0582</td>\n",
       "<td> (26.0/447.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>80.0</td>\n",
       "<td>185.0</td>\n",
       "<td>0.3019</td>\n",
       "<td> (80.0/265.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>501.0</td>\n",
       "<td>211.0</td>\n",
       "<td>0.1489</td>\n",
       "<td> (106.0/712.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      421  26   0.0582   (26.0/447.0)\n",
       "1      80   185  0.3019   (80.0/265.0)\n",
       "Total  501  211  0.1489   (106.0/712.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2334152</td>\n",
       "<td>0.7773109</td>\n",
       "<td>170.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0290583</td>\n",
       "<td>0.8062284</td>\n",
       "<td>302.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2885967</td>\n",
       "<td>0.8357075</td>\n",
       "<td>156.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.2334152</td>\n",
       "<td>0.8511236</td>\n",
       "<td>170.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9996985</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0026000</td>\n",
       "<td>1.0</td>\n",
       "<td>392.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9996985</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2334152</td>\n",
       "<td>0.6774248</td>\n",
       "<td>170.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0702982</td>\n",
       "<td>0.8113208</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1284378</td>\n",
       "<td>0.8214976</td>\n",
       "<td>211.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.233415     0.777311  170\n",
       "max f2                       0.0290583    0.806228  302\n",
       "max f0point5                 0.288597     0.835708  156\n",
       "max accuracy                 0.233415     0.851124  170\n",
       "max precision                0.999698     1         0\n",
       "max recall                   0.00260001   1         392\n",
       "max specificity              0.999698     1         0\n",
       "max absolute_mcc             0.233415     0.677425  170\n",
       "max min_per_class_accuracy   0.0702982    0.811321  246\n",
       "max mean_per_class_accuracy  0.128438     0.821498  211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 37.22 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0112360</td>\n",
       "<td>0.9958643</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0301887</td>\n",
       "<td>0.0301887</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0210674</td>\n",
       "<td>0.9937712</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0566038</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0308989</td>\n",
       "<td>0.9922370</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.0830189</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0407303</td>\n",
       "<td>0.9900050</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1094340</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0505618</td>\n",
       "<td>0.9847492</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.1358491</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1011236</td>\n",
       "<td>0.9513030</td>\n",
       "<td>2.6867925</td>\n",
       "<td>2.6867925</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1358491</td>\n",
       "<td>0.2716981</td>\n",
       "<td>168.6792453</td>\n",
       "<td>168.6792453</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1502809</td>\n",
       "<td>0.9090890</td>\n",
       "<td>2.3797305</td>\n",
       "<td>2.5863516</td>\n",
       "<td>0.8857143</td>\n",
       "<td>0.9626168</td>\n",
       "<td>0.1169811</td>\n",
       "<td>0.3886792</td>\n",
       "<td>137.9730458</td>\n",
       "<td>158.6351613</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2008427</td>\n",
       "<td>0.7460694</td>\n",
       "<td>2.3136268</td>\n",
       "<td>2.5176936</td>\n",
       "<td>0.8611111</td>\n",
       "<td>0.9370629</td>\n",
       "<td>0.1169811</td>\n",
       "<td>0.5056604</td>\n",
       "<td>131.3626834</td>\n",
       "<td>151.7693627</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3005618</td>\n",
       "<td>0.2245415</td>\n",
       "<td>1.9299495</td>\n",
       "<td>2.3226944</td>\n",
       "<td>0.7183099</td>\n",
       "<td>0.8644860</td>\n",
       "<td>0.1924528</td>\n",
       "<td>0.6981132</td>\n",
       "<td>92.9949508</td>\n",
       "<td>132.2694410</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4002809</td>\n",
       "<td>0.0826768</td>\n",
       "<td>0.9838958</td>\n",
       "<td>1.9891691</td>\n",
       "<td>0.3661972</td>\n",
       "<td>0.7403509</td>\n",
       "<td>0.0981132</td>\n",
       "<td>0.7962264</td>\n",
       "<td>-1.6104172</td>\n",
       "<td>98.9169149</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0387548</td>\n",
       "<td>0.5297901</td>\n",
       "<td>1.6981132</td>\n",
       "<td>0.1971831</td>\n",
       "<td>0.6320225</td>\n",
       "<td>0.0528302</td>\n",
       "<td>0.8490566</td>\n",
       "<td>-47.0209939</td>\n",
       "<td>69.8113208</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5997191</td>\n",
       "<td>0.0221373</td>\n",
       "<td>0.3784215</td>\n",
       "<td>1.4786797</td>\n",
       "<td>0.1408451</td>\n",
       "<td>0.5503513</td>\n",
       "<td>0.0377358</td>\n",
       "<td>0.8867925</td>\n",
       "<td>-62.1578528</td>\n",
       "<td>47.8679687</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7022472</td>\n",
       "<td>0.0174328</td>\n",
       "<td>0.2576376</td>\n",
       "<td>1.3004075</td>\n",
       "<td>0.0958904</td>\n",
       "<td>0.484</td>\n",
       "<td>0.0264151</td>\n",
       "<td>0.9132075</td>\n",
       "<td>-74.2362368</td>\n",
       "<td>30.0407547</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8019663</td>\n",
       "<td>0.0126055</td>\n",
       "<td>0.4541058</td>\n",
       "<td>1.1951756</td>\n",
       "<td>0.1690141</td>\n",
       "<td>0.4448336</td>\n",
       "<td>0.0452830</td>\n",
       "<td>0.9584906</td>\n",
       "<td>-54.5894233</td>\n",
       "<td>19.5175627</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8988764</td>\n",
       "<td>0.0094713</td>\n",
       "<td>0.2336341</td>\n",
       "<td>1.0915094</td>\n",
       "<td>0.0869565</td>\n",
       "<td>0.40625</td>\n",
       "<td>0.0226415</td>\n",
       "<td>0.9811321</td>\n",
       "<td>-76.6365874</td>\n",
       "<td>9.1509434</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000126</td>\n",
       "<td>0.1865828</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0694444</td>\n",
       "<td>0.3721910</td>\n",
       "<td>0.0188679</td>\n",
       "<td>1.0</td>\n",
       "<td>-81.3417191</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.011236                    0.995864           2.68679   2.68679            1                1                           0.0301887       0.0301887                  168.679   168.679\n",
       "    2        0.0210674                   0.993771           2.68679   2.68679            1                1                           0.0264151       0.0566038                  168.679   168.679\n",
       "    3        0.0308989                   0.992237           2.68679   2.68679            1                1                           0.0264151       0.0830189                  168.679   168.679\n",
       "    4        0.0407303                   0.990005           2.68679   2.68679            1                1                           0.0264151       0.109434                   168.679   168.679\n",
       "    5        0.0505618                   0.984749           2.68679   2.68679            1                1                           0.0264151       0.135849                   168.679   168.679\n",
       "    6        0.101124                    0.951303           2.68679   2.68679            1                1                           0.135849        0.271698                   168.679   168.679\n",
       "    7        0.150281                    0.909089           2.37973   2.58635            0.885714         0.962617                    0.116981        0.388679                   137.973   158.635\n",
       "    8        0.200843                    0.746069           2.31363   2.51769            0.861111         0.937063                    0.116981        0.50566                    131.363   151.769\n",
       "    9        0.300562                    0.224541           1.92995   2.32269            0.71831          0.864486                    0.192453        0.698113                   92.995    132.269\n",
       "    10       0.400281                    0.0826768          0.983896  1.98917            0.366197         0.740351                    0.0981132       0.796226                   -1.61042  98.9169\n",
       "    11       0.5                         0.0387548          0.52979   1.69811            0.197183         0.632022                    0.0528302       0.849057                   -47.021   69.8113\n",
       "    12       0.599719                    0.0221373          0.378421  1.47868            0.140845         0.550351                    0.0377358       0.886792                   -62.1579  47.868\n",
       "    13       0.702247                    0.0174328          0.257638  1.30041            0.0958904        0.484                       0.0264151       0.913208                   -74.2362  30.0408\n",
       "    14       0.801966                    0.0126055          0.454106  1.19518            0.169014         0.444834                    0.045283        0.958491                   -54.5894  19.5176\n",
       "    15       0.898876                    0.00947127         0.233634  1.09151            0.0869565        0.40625                     0.0226415       0.981132                   -76.6366  9.15094\n",
       "    16       1                           1.26445e-05        0.186583  1                  0.0694444        0.372191                    0.0188679       1                          -81.3417  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>training_speed</b></td>\n",
       "<td><b>epochs</b></td>\n",
       "<td><b>iterations</b></td>\n",
       "<td><b>samples</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:12</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>None</td>\n",
       "<td>0.0</td>\n",
       "<td>0</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:13</td>\n",
       "<td> 1.451 sec</td>\n",
       "<td>2770 obs/sec</td>\n",
       "<td>1.0</td>\n",
       "<td>1</td>\n",
       "<td>712.0</td>\n",
       "<td>0.4390029</td>\n",
       "<td>1.2340877</td>\n",
       "<td>0.8447216</td>\n",
       "<td>2.3509434</td>\n",
       "<td>0.2022472</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2017-06-29 23:36:14</td>\n",
       "<td> 2.503 sec</td>\n",
       "<td>5545 obs/sec</td>\n",
       "<td>10.0</td>\n",
       "<td>10</td>\n",
       "<td>7120.0</td>\n",
       "<td>0.3767767</td>\n",
       "<td>0.5042119</td>\n",
       "<td>0.8779030</td>\n",
       "<td>2.6867925</td>\n",
       "<td>0.1488764</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    training_speed    epochs    iterations    samples    training_rmse    training_logloss    training_auc    training_lift    training_classification_error\n",
       "--  -------------------  ----------  ----------------  --------  ------------  ---------  ---------------  ------------------  --------------  ---------------  -------------------------------\n",
       "    2017-06-29 23:36:12  0.000 sec                     0         0             0          nan              nan                 nan             nan              nan\n",
       "    2017-06-29 23:36:13  1.451 sec   2770 obs/sec      1         1             712        0.439003         1.23409             0.844722        2.35094          0.202247\n",
       "    2017-06-29 23:36:14  2.503 sec   5545 obs/sec      10        10            7120       0.376777         0.504212            0.877903        2.68679          0.148876"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>Sex.male</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0882838</td></tr>\n",
       "<tr><td>Embarked.S</td>\n",
       "<td>0.9970510</td>\n",
       "<td>0.9970510</td>\n",
       "<td>0.0880234</td></tr>\n",
       "<tr><td>Sex.female</td>\n",
       "<td>0.9387698</td>\n",
       "<td>0.9387698</td>\n",
       "<td>0.0828781</td></tr>\n",
       "<tr><td>Pclass.2</td>\n",
       "<td>0.9264013</td>\n",
       "<td>0.9264013</td>\n",
       "<td>0.0817862</td></tr>\n",
       "<tr><td>Pclass.3</td>\n",
       "<td>0.8978523</td>\n",
       "<td>0.8978523</td>\n",
       "<td>0.0792658</td></tr>\n",
       "<tr><td>Pclass.1</td>\n",
       "<td>0.8794286</td>\n",
       "<td>0.8794286</td>\n",
       "<td>0.0776393</td></tr>\n",
       "<tr><td>Embarked.Q</td>\n",
       "<td>0.8743024</td>\n",
       "<td>0.8743024</td>\n",
       "<td>0.0771867</td></tr>\n",
       "<tr><td>Embarked.missing(NA)</td>\n",
       "<td>0.8721952</td>\n",
       "<td>0.8721952</td>\n",
       "<td>0.0770007</td></tr>\n",
       "<tr><td>Embarked.C</td>\n",
       "<td>0.8366063</td>\n",
       "<td>0.8366063</td>\n",
       "<td>0.0738588</td></tr>\n",
       "<tr><td>Age</td>\n",
       "<td>0.8122383</td>\n",
       "<td>0.8122383</td>\n",
       "<td>0.0717075</td></tr>\n",
       "<tr><td>SibSp</td>\n",
       "<td>0.7825306</td>\n",
       "<td>0.7825306</td>\n",
       "<td>0.0690848</td></tr>\n",
       "<tr><td>Fare</td>\n",
       "<td>0.7706785</td>\n",
       "<td>0.7706785</td>\n",
       "<td>0.0680384</td></tr>\n",
       "<tr><td>Parch</td>\n",
       "<td>0.7390545</td>\n",
       "<td>0.7390545</td>\n",
       "<td>0.0652465</td></tr>\n",
       "<tr><td>Pclass.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>Sex.missing(NA)</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "variable              relative_importance    scaled_importance    percentage\n",
       "--------------------  ---------------------  -------------------  ------------\n",
       "Sex.male              1                      1                    0.0882838\n",
       "Embarked.S            0.997051               0.997051             0.0880234\n",
       "Sex.female            0.93877                0.93877              0.0828781\n",
       "Pclass.2              0.926401               0.926401             0.0817862\n",
       "Pclass.3              0.897852               0.897852             0.0792658\n",
       "Pclass.1              0.879429               0.879429             0.0776393\n",
       "Embarked.Q            0.874302               0.874302             0.0771867\n",
       "Embarked.missing(NA)  0.872195               0.872195             0.0770007\n",
       "Embarked.C            0.836606               0.836606             0.0738588\n",
       "Age                   0.812238               0.812238             0.0717075\n",
       "SibSp                 0.782531               0.782531             0.0690848\n",
       "Fare                  0.770678               0.770678             0.0680384\n",
       "Parch                 0.739055               0.739055             0.0652465\n",
       "Pclass.missing(NA)    0                      0                    0\n",
       "Sex.missing(NA)       0                      0                    0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the DNN model summary\n",
    "dnn_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.150622120984378\n",
      "RMSE: 0.38810065831479595\n",
      "LogLoss: 0.5304682785258303\n",
      "Mean Per-Class Error: 0.16259230965113325\n",
      "AUC: 0.890183346065699\n",
      "Gini: 0.7803666921313981\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.16554549204423366: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>94.0</td>\n",
       "<td>8.0</td>\n",
       "<td>0.0784</td>\n",
       "<td> (8.0/102.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>19.0</td>\n",
       "<td>58.0</td>\n",
       "<td>0.2468</td>\n",
       "<td> (19.0/77.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>113.0</td>\n",
       "<td>66.0</td>\n",
       "<td>0.1508</td>\n",
       "<td> (27.0/179.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  ------------\n",
       "0      94   8    0.0784   (8.0/102.0)\n",
       "1      19   58   0.2468   (19.0/77.0)\n",
       "Total  113  66   0.1508   (27.0/179.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1655455</td>\n",
       "<td>0.8111888</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0320367</td>\n",
       "<td>0.8455882</td>\n",
       "<td>95.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4103082</td>\n",
       "<td>0.8637874</td>\n",
       "<td>52.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.1655455</td>\n",
       "<td>0.8491620</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9965904</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0082606</td>\n",
       "<td>1.0</td>\n",
       "<td>148.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9965904</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1655455</td>\n",
       "<td>0.6924991</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1132890</td>\n",
       "<td>0.8051948</td>\n",
       "<td>76.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1655455</td>\n",
       "<td>0.8374077</td>\n",
       "<td>62.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.165545     0.811189  62\n",
       "max f2                       0.0320367    0.845588  95\n",
       "max f0point5                 0.410308     0.863787  52\n",
       "max accuracy                 0.165545     0.849162  62\n",
       "max precision                0.99659      1         0\n",
       "max recall                   0.00826058   1         148\n",
       "max specificity              0.99659      1         0\n",
       "max absolute_mcc             0.165545     0.692499  62\n",
       "max min_per_class_accuracy   0.113289     0.805195  76\n",
       "max mean_per_class_accuracy  0.165545     0.837408  62"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 43.02 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0111732</td>\n",
       "<td>0.9957765</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0259740</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0223464</td>\n",
       "<td>0.9936156</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0519481</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0335196</td>\n",
       "<td>0.9897613</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.0779221</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0446927</td>\n",
       "<td>0.9888479</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.1038961</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0502793</td>\n",
       "<td>0.9866998</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.3246753</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1168831</td>\n",
       "<td>132.4675325</td>\n",
       "<td>132.4675325</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1005587</td>\n",
       "<td>0.9681773</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1955267</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.2207792</td>\n",
       "<td>106.6378066</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1508380</td>\n",
       "<td>0.9345274</td>\n",
       "<td>2.3246753</td>\n",
       "<td>2.2385762</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9629630</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.3376623</td>\n",
       "<td>132.4675325</td>\n",
       "<td>123.8576239</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2011173</td>\n",
       "<td>0.9167866</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1955267</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.1038961</td>\n",
       "<td>0.4415584</td>\n",
       "<td>106.6378066</td>\n",
       "<td>119.5526696</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3016760</td>\n",
       "<td>0.4493311</td>\n",
       "<td>2.0663781</td>\n",
       "<td>2.1524772</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9259259</td>\n",
       "<td>0.2077922</td>\n",
       "<td>0.6493506</td>\n",
       "<td>106.6378066</td>\n",
       "<td>115.2477152</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4022346</td>\n",
       "<td>0.1294009</td>\n",
       "<td>1.1623377</td>\n",
       "<td>1.9049423</td>\n",
       "<td>0.5</td>\n",
       "<td>0.8194444</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.7662338</td>\n",
       "<td>16.2337662</td>\n",
       "<td>90.4942280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5027933</td>\n",
       "<td>0.0684903</td>\n",
       "<td>0.9040404</td>\n",
       "<td>1.7047619</td>\n",
       "<td>0.3888889</td>\n",
       "<td>0.7333333</td>\n",
       "<td>0.0909091</td>\n",
       "<td>0.8571429</td>\n",
       "<td>-9.5959596</td>\n",
       "<td>70.4761905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5977654</td>\n",
       "<td>0.0258376</td>\n",
       "<td>0.4102368</td>\n",
       "<td>1.4990897</td>\n",
       "<td>0.1764706</td>\n",
       "<td>0.6448598</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.8961039</td>\n",
       "<td>-58.9763178</td>\n",
       "<td>49.9089695</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6983240</td>\n",
       "<td>0.0189629</td>\n",
       "<td>0.3874459</td>\n",
       "<td>1.3390130</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.576</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-61.2554113</td>\n",
       "<td>33.9012987</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7988827</td>\n",
       "<td>0.0127011</td>\n",
       "<td>0.2582973</td>\n",
       "<td>1.2029788</td>\n",
       "<td>0.1111111</td>\n",
       "<td>0.5174825</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.9610390</td>\n",
       "<td>-74.1702742</td>\n",
       "<td>20.2978839</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8994413</td>\n",
       "<td>0.0085157</td>\n",
       "<td>0.2582973</td>\n",
       "<td>1.0973623</td>\n",
       "<td>0.1111111</td>\n",
       "<td>0.4720497</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.9870130</td>\n",
       "<td>-74.1702742</td>\n",
       "<td>9.7362265</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000665</td>\n",
       "<td>0.1291486</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.4301676</td>\n",
       "<td>0.0129870</td>\n",
       "<td>1.0</td>\n",
       "<td>-87.0851371</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0111732                   0.995776           2.32468   2.32468            1                1                           0.025974        0.025974                   132.468   132.468\n",
       "    2        0.0223464                   0.993616           2.32468   2.32468            1                1                           0.025974        0.0519481                  132.468   132.468\n",
       "    3        0.0335196                   0.989761           2.32468   2.32468            1                1                           0.025974        0.0779221                  132.468   132.468\n",
       "    4        0.0446927                   0.988848           2.32468   2.32468            1                1                           0.025974        0.103896                   132.468   132.468\n",
       "    5        0.0502793                   0.9867             2.32468   2.32468            1                1                           0.012987        0.116883                   132.468   132.468\n",
       "    6        0.100559                    0.968177           2.06638   2.19553            0.888889         0.944444                    0.103896        0.220779                   106.638   119.553\n",
       "    7        0.150838                    0.934527           2.32468   2.23858            1                0.962963                    0.116883        0.337662                   132.468   123.858\n",
       "    8        0.201117                    0.916787           2.06638   2.19553            0.888889         0.944444                    0.103896        0.441558                   106.638   119.553\n",
       "    9        0.301676                    0.449331           2.06638   2.15248            0.888889         0.925926                    0.207792        0.649351                   106.638   115.248\n",
       "    10       0.402235                    0.129401           1.16234   1.90494            0.5              0.819444                    0.116883        0.766234                   16.2338   90.4942\n",
       "    11       0.502793                    0.0684903          0.90404   1.70476            0.388889         0.733333                    0.0909091       0.857143                   -9.59596  70.4762\n",
       "    12       0.597765                    0.0258376          0.410237  1.49909            0.176471         0.64486                     0.038961        0.896104                   -58.9763  49.909\n",
       "    13       0.698324                    0.0189629          0.387446  1.33901            0.166667         0.576                       0.038961        0.935065                   -61.2554  33.9013\n",
       "    14       0.798883                    0.0127011          0.258297  1.20298            0.111111         0.517483                    0.025974        0.961039                   -74.1703  20.2979\n",
       "    15       0.899441                    0.00851574         0.258297  1.09736            0.111111         0.47205                     0.025974        0.987013                   -74.1703  9.73623\n",
       "    16       1                           6.64904e-05        0.129149  1                  0.0555556        0.430168                    0.012987        1                          -87.0851  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model performance on test dataset\n",
    "dnn_default.model_performance(titanic_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">      p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.099139</td><td style=\"text-align: right;\">0.900861</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.110799</td><td style=\"text-align: right;\">0.889201</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.262639</td><td style=\"text-align: right;\">0.737361</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.789631</td><td style=\"text-align: right;\">0.210369</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.482829</td><td style=\"text-align: right;\">0.517171</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GLM model to make predictions\n",
    "yhat_test_glm = glm_default.predict(titanic_test)\n",
    "yhat_test_glm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">         p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.000425532</td><td style=\"text-align: right;\">0.999574 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0696667  </td><td style=\"text-align: right;\">0.930333 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.254742   </td><td style=\"text-align: right;\">0.745258 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.926995   </td><td style=\"text-align: right;\">0.0730054</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.442731   </td><td style=\"text-align: right;\">0.557269 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use DRF model to make predictions\n",
    "yhat_test_drf = drf_default.predict(titanic_test)\n",
    "yhat_test_drf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">       p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0146597</td><td style=\"text-align: right;\">0.98534  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0138361</td><td style=\"text-align: right;\">0.986164 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.117054 </td><td style=\"text-align: right;\">0.882946 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.922287 </td><td style=\"text-align: right;\">0.0777134</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.438084 </td><td style=\"text-align: right;\">0.561916 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GBM model to make predictions\n",
    "yhat_test_gbm = gbm_default.predict(titanic_test)\n",
    "yhat_test_gbm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning prediction progress: |███████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">       p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.02934  </td><td style=\"text-align: right;\">0.97066  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.0536184</td><td style=\"text-align: right;\">0.946382 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.495336 </td><td style=\"text-align: right;\">0.504664 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.980656 </td><td style=\"text-align: right;\">0.0193439</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.779117 </td><td style=\"text-align: right;\">0.220883 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use DNN model to make predictions\n",
    "yhat_test_dnn = dnn_default.predict(titanic_test)\n",
    "yhat_test_dnn.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
